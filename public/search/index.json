[{"content":"应用程序的一次执行过程称为一个任务。\n在 BTD-OS 中我们将进程和线程统一用任务控制块 (TaskContralBlock) 结构表示，维护着操作系统对于任务的管理信息，结构如下：\n// branch-main: kernel/src/task/task.rs pub struct TaskControlBlock { // immutable pub pid: PidHandle, pub tgid: usize, pub kernel_stack: KernelStack, // mutable according to clone flags pub sigactions: Arc\u0026lt;RwLock\u0026lt;[SigAction; MAX_SIGNUM as usize]\u0026gt;\u0026gt;, pub memory_set: Arc\u0026lt;RwLock\u0026lt;MemorySet\u0026gt;\u0026gt;, pub fd_table: Arc\u0026lt;RwLock\u0026lt;FDTable\u0026gt;\u0026gt;, // mutable inner: RwLock\u0026lt;TaskControlBlockInner\u0026gt;, } pid / tgid 由全局分配器 PID_ALLOCATOR 分配，并且分配的 ID 仅增长不回收，因为 BTD-OS 实现线程的过程中，默认认为 tgid 不小于 pid； 若该任务控制块为进程，则 TaskControlBlock 的 pid 与 tgid 字段值相同； pid 与 tgid 的差值可以用于计算线程 TrapContext 虚拟地址； kernel_stack 字段仅用于创建时分配内核栈，故无需修改； sigactions, memory_set, fd_table 字段均需要根据 fork 时的 CloneFlags 参数结合 man-page 要求创建； pub struct TaskControlBlockInner { pub trap_cx_ppn: PhysPageNum, pub task_cx: TaskContext, pub task_status: TaskStatus, pub trap_cause: Option\u0026lt;Scause\u0026gt;, pub parent: Option\u0026lt;Weak\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, // child process and thread collection pub children: Vec\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, pub pending_signals: SigSet, pub sigmask: SigMask, pub cwd: AbsolutePath, pub exit_code: i32, pub interval_timer: Option\u0026lt;IntervalTimer\u0026gt;, pub utime: TimeVal, pub stime: TimeVal, pub last_enter_umode_time: TimeVal, pub last_enter_smode_time: TimeVal, pub robust_list: RobustList, pub rlimit_nofile: RLimit, pub clear_child_tid: usize, /* CLONE_CHILD_CLEARTID */ } trap_cause 用于记录 TCB 进入 trap_handler 的原因，BTD-OS 具体用于时钟中断更新时间片的问题（后文会介绍 BTD-OS 为何改进时钟中断处理）； parent 表示可能存在的父进程； children 用于收集 fork 时创建的子进程/子线程的 Arc (原子引用计数)； pending_signals 表示待处理的信号集； sigmask 表示被屏蔽的信号集； interval_timer 用于处理与定时器相关的系统调用； utime, stime, last_enter_umode_time, last_enter_smode_time 用于记录 TCB 分别在 U Mode 和 S Mode 下所耗费的时间，用于处理 getrusage 系统调用； robust_list 用于实现 get_robust_list，set_robust_list 系统调用 rlimit_nofile 用于实现 rlimit 相关系统调用，但目前仅用于限制进程可打开的文件数 clear_child_tid 用于实现 fork，set_tid_address 系统调用 另外，我们在参考往届优秀作品实现时，有注意到有些队伍在 TCB 中保留了 user_stask 字段。该字段用于保存为线程分配用户栈时保留的 user_stack 虚拟地址。\nBTD-OS 的 TCB 中并没有保存，原因是按照 man-page 关于 fork(clone) 系统调用的规定对于共享内存的子进程或子线程，创建传入 stask 参数，该参数规定了子进程/线程的用户栈的位置，故实际上不需要我们额外创建 user_stack。\n对于非 forK(clone) 创建的进程（new/exec load_elf 方法中），BTD-OS 会分配并映射该进程的 user_stack。\n// kernel/src/mm/memory_set.rs: fn load_elf memory_set.user_stack_areas = VmArea::new( user_stack_bottom.into(), user_stack_top.into(), MapType::Framed, VmAreaType::UserStack, MapPermission::R | MapPermission::W | MapPermission::U, None, 0, ); ","date":"2023-08-19","permalink":"https://bitethedisk.github.io/post/task_control_block/","tags":null,"title":"任务控制块"},{"content":"RISC-V架构定义了4种特权级别，其中只有M模式是必须实现的，其余特权级别可以根据运行在CPU上的应用需求进行调整。\nBTD OS涉及M/S/U三种特权级别，可以根据运行时的特权级别分为三个部分：\n用户/应用模式 (U, User/Application)：用户应用程序运行在 U 态。具有最低限度的权限，无法读写特权级寄存器，也无法修改S态和M态的内存空间，只能通过系统调用或异常中断等方式与S态进行交互。 监督模式 (S, Supervisor)：操作系统的内核运行在 S 态，是用户程序的运行环境。在这个模式下，内核需要协调调度进程、响应用户的系统调用请求、管理进程的内存空间以及控制设备的IO操作等。 机器模式 (M, Machine)：监督模式执行环境 (SEE, Supervisor Execution Environment) 运行在 M 模式上，负责完成与硬件相关的初始化等工作，如在操作系统运行前负责加载操作系统的 Bootloader – RustSBI，是操作系统内核的运行环境。 在 RISC-V 中，各模式使用 ecall 指令与对应模式下的 ret 指令进行特权级切换，对于用户模式（即用户态）切 换到监督模式（即内核态），主要是使用内核提供的提供了 ABI（Application Binary Interface）接口，即 BTD OS 实现的系统调用。对于监督模式切换到机器模式，我们选择使用 OpenSBI，它是运行在更底层的 M 模式特权级下的软件，实现了对硬件的控制，如串口输入输出、多核心启动、操作系统的关机等操作，是操作系统内核的执行环境。\n! 目录树介绍 src ├── board │ ├── fu740.rs │ ├── mod.rs │ └── qemu.rs ├── boards │ └── qemu.rs ├── console.rs ├── consts.rs ├── drivers │ ├── fu740 │ │ ├── mod.rs │ │ ├── plic.rs │ │ ├── prci.rs │ │ ├── sdcard.rs │ │ └── spi.rs │ ├── mod.rs │ └── qemu │ ├── mod.rs │ ├── virtio_blk.rs │ └── virtio_impl.rs ├── entry.S ├── fs │ ├── fat32 │ │ ├── file.rs │ │ └── mod.rs │ ├── file.rs │ ├── mod.rs │ ├── mount.rs │ ├── page.rs │ ├── page_cache.rs │ ├── pipe.rs │ └── stdio │ ├── mod.rs │ ├── stdin.rs │ └── stdout.rs ├── logging.rs ├── macros │ ├── hsm.rs │ ├── mod.rs │ ├── on_boot.rs │ └── profile.rs ├── main.rs ├── mm │ ├── address.rs │ ├── frame_allocator.rs │ ├── kernel_heap_allocator.rs │ ├── kvmm.rs │ ├── memory_set.rs │ ├── mmap.rs │ ├── mod.rs │ ├── page_table.rs │ ├── permission.rs │ ├── shared_memory.rs │ ├── user_buffer.rs │ └── vm_area.rs ├── panic.rs ├── sbi.rs ├── syscall │ ├── dispatcher.rs │ ├── errno.rs │ ├── futex.rs │ ├── impls │ │ ├── fs.rs │ │ ├── futex.rs │ │ ├── mm.rs │ │ ├── mod.rs │ │ ├── others.rs │ │ └── process.rs │ └── mod.rs ├── task │ ├── context.rs │ ├── id.rs │ ├── initproc │ │ ├── busybox │ │ ├── initproc.S │ │ ├── mod.rs │ │ └── test_all_custom.sh │ ├── kstack.rs │ ├── manager │ │ ├── hanging_task.rs │ │ ├── mod.rs │ │ └── task_manager.rs │ ├── mod.rs │ ├── processor │ │ ├── mod.rs │ │ ├── processor.rs │ │ └── schedule.rs │ ├── signal.rs │ ├── switch │ │ ├── mod.rs │ │ └── switch.S │ └── task.rs ├── timer.rs └── trap ├── context.rs ├── handler.rs ├── mod.rs └── trampoline.S ","date":"2023-08-19","permalink":"https://bitethedisk.github.io/post/overall_architecture/","tags":null,"title":"系统设计与整体架构"},{"content":"BTD-OS是一款基于 Rust 语言开发的宏内核操作系统，适用于 RISC-V64 平台。\n它实现了中断与异常处理、进程管理、内存管理、文件系统和信号系统等操作系统基本模块。目前，BTD-OS 可以在 QEMU 虚拟环境和 HiFive Unmatched U740 板卡上运行。\nBTD-OS 的核心目标是设计一款简洁、小巧、结构清晰、功能完善且具有良好可拓展性的操作系统。BiteTheDisk 团队在代码和文档中尽可能地保留了 BTD-OS 的设计与实现思路，包括问题与解决方案以及后续可能的优化方案，以展现操作系统的基本功能和核心思想，方便他人学习和借鉴，加深对操作系统基本原理与实现的理解。\n编程语言的选择 虽然常见的操作系统内核都是基于 C 语言的，但 BTD-OS 使用 Rust 语言进行开发，原因如下：\n内存安全性：Rust 是一种内存安全的编程语言，其借用检查器（Borrow Checker）可以在编译时捕获内存访问错误，避免了常见的内存安全问题，如空指针引用、数据竞争等。 性能：Rust 语言的设计目标之一是提供与 C 和 C++ 相媲美的性能。它具有零开销抽象、内联汇编和对底层硬件的直接访问等特性，使得开发人员能够编写高效的系统级代码。 可靠性：Rust 鼓励编写可靠的代码。其强制的所有权和借用规则以及丰富的静态类型系统可以在编译时捕获潜在的错误，减少运行时错误的可能性。 丰富的第三方库支持：Rust生态系统中有大量的高质量第三方库，可以帮助简化内核开发过程。 尽管C语言是广泛用于操作系统开发的传统语言，但 Rust 作为一种现代且创新的语言，为开发者提供了更好的工具和保证，有助于编写更安全、可靠和高效的操作系统代码。\n开发历程 在比赛准备之前，我们的团队中的同学已经对 Rust 语言有初步的接触。恰好在学习 Rust 的过程中，我们发现了一个非常好的 Rust 入门项目，即 rCore-Tutorial-v3。于是我们参加了 rCore-Tutorial 的操作系统训练营。在开源社区的朋友们的帮助下，我们完成了部分实验，进一步加深了对操作系统基本原理以及 rCore 部分实现细节的理解。\n在区域赛阶段，我们先简单阅读了部分往届优秀作品如 FTL-OS, NPUCore-OS, OopsOS, RongOS, JKXS-OS 等等。虽然这些优秀作品中，特别是获得了全国赛一等奖的作品，都有各自优秀的，独特的设计与优化方案，但这些设计与优化对于刚刚从 rCore-Tutorial 学习后的我们来说，理解起来比较吃力。于是在 BTD-OS 设计初期选择了与 rCore-Tutorail 设计比较贴合的 RongOS 作为参考模板，结合 rCore-Tutorial-v3 的 ch5 分支开发，尽可能地使内核看起来结构清晰，简洁小巧。\n在全国赛第一阶段，主要工作是是移植 busybox 完善各个测试要求的系统调用以及接入硬件设备 HiFive Unmatched U740 板卡。在这个时期，我们花费了大量的时间根据 man-page 文档要求，完善初期要求实现的系统调用，添加新的系统调用，并在内核中加入这些系统调用要求维护的数据结构，参考往届作品中对硬件设备的移植方案等等。到第一阶段提交时，我们完成了大部分的测例，但仍有部分测例要求的结构与功能，如网络等，在内核中还未实现。另外，由于在线硬件资源较难获取，我们没有完成对于硬件接入的调试。同时，在QEMU平台上运行测试时，我们发现内核的执行速度非常慢，难以接受。于是在在全国赛第一阶段结束后，我们通过自己写的一些小的性能测试工具，如 time-tracer 发现了内核最大的性能问题 —— 大量地直接读写 FAT32 文件系统。\n在进入全国赛第二阶段前，我们主要工作包括：通过引入 PageCache 机制，改进 FAT32 读写过程中的簇链查找模式以解决文件系统导致的性能问题；完善对硬件设备 HiFive Unmatched U740 板卡的支持；改进多核的支持；完成网络设计等工作。\n完成情况 内核模块 完成情况 系统调用 进程管理 分时多任务\n多线程\n多核\n符合 man-page 规范的 futex 机制 clone, exec, wait4, exit,\nexit_group, getpid, getppid, gettid,\nset_tid_address, clock_gettime,\nkill, tkill, getscheduler\nclock_getres, socketpair,\nset_robust_list, get_robust_list,\nprlimit64 等 内存管理 页缓存\n懒分配与写时拷贝\n共享内存 brk, munmap, mmap, shmget,\nshmctl, shmat, shmdt, mprotect 文件系统 FAT32\n块缓存\n与内存页缓存结合\nInodeCache getcwd, pipe2, dup, dup3,\nchdir, openat, close, getdents64,\nread, pread64, write, pwrite64,\nunlinkat, mkdirat, ummount2, mount,\nfstat, readv, writev, ioctl,\nfcntl, newfstatat, sendfile,\nutimensat, renameat2, lseek,\nreadlinkat, sync, ftruncate64,\npselect, statfs 等 信号系统 符合 man-page 规范的信号处理机制 sigreturn, sigaction, sigprocmask 其他系统调用 times, uname, sched_yield, gettimeofday\nnanosleep, clock_nanosleep, gettrandom,\nsettimer, getittimer, timer_settime,\nrecvfrom ","date":"2023-08-19","permalink":"https://bitethedisk.github.io/post/overview/","tags":["Docs"],"title":"[Docs] 概述"},{"content":"我们在 FAT32 设计上，采用了 rCore-Tutorial easy-fs 相同的松耦合模块化设计思路，与底层设备驱动之间通过抽象接口 BlockDevice 来连接，避免了与设备驱动的绑定。FAT32 库通过 Rust 提供的 alloc crate 来隔离了操作系统内核的内存管理，避免了直接调用内存管理的内核函数。 同时在设计中避免了直接访问进程相关的数据和函数，从而隔离了操作系统内核的进程管理。\n虽然我们在内核中给出了虚拟文件的抽象 File Trait，但是由于只完成的 FAT32 文件系统，内核中不存在虚拟文件系统这一抽象，也没有 Inode 层面的缓存，内核中的文件实际上将 FAT32 提供的 VirtFile 进一步封装成 KFile，这导致每次写入数据都会同步到磁盘，此时 BlockCache 缓存块的作用微乎其微。\n第一阶段 —— Static BusyBox 在全国赛第一阶段时，我们在测试过程中发现内核跑测例的执行速度非常缓慢， 比如启动 busybox 都需要花半分钟、iozone 测试中，读写速度大概为几十到几百 KB/s，显然内核的文件读写性能非常糟糕，导致没法跑完我们已经实现的测试。\n此时为了尽可能跑完测试，我们发现由于大部分测试需要使用 busybox，为了避免多次解析 elf、 从零创建地址空间等问题，我们采用了类似于加载 initproc 的方法。 具体而言，我们将 busybox 预加载到内核中，并保存 load_elf 获取的信息。每次执行 busybox 时， 我们直接使用保存的 load_elf 信息，并通过写时拷贝来创建所需的 busybox 进程的地址空间，更快速地创建 busybox。\n// 第一阶段 submit 分支 pub static ref BUSYBOX: RwLock\u0026lt;Busybox\u0026gt; = RwLock::new({ extern \u0026quot;C\u0026quot; { fn busybox_entry(); fn busybox_tail(); } let entry = busybox_entry as usize; let tail = busybox_tail as usize; let siz = tail - entry; let busybox = unsafe { core::slice::from_raw_parts(entry as *const u8, siz) }; let path = AbsolutePath::from_str(\u0026quot;/busybox0\u0026quot;); let inode = fs::open(path, OpenFlags::O_CREATE, CreateMode::empty()).expect(\u0026quot;busybox0 create failed\u0026quot;); inode.write_all(\u0026amp;busybox.to_owned()); let bb = Arc::new(TaskControlBlock::new(inode.clone())); inode.delete(); Busybox { inner: bb, } }); pub static mut ONCE_BB_ENTRY: usize = 0; pub static mut ONCE_BB_AUX: Vec\u0026lt;AuxEntry\u0026gt; = Vec::new(); pub struct Busybox { inner: Arc\u0026lt;TaskControlBlock\u0026gt;, } impl Busybox { pub fn elf_entry_point(\u0026amp;self) -\u0026gt; usize { unsafe { ONCE_BB_ENTRY } } pub fn aux(\u0026amp;self) -\u0026gt; Vec\u0026lt;AuxEntry\u0026gt; { unsafe { ONCE_BB_AUX.clone() } } pub fn memory_set(\u0026amp;self) -\u0026gt; MemorySet { let mut write = self.inner.memory_set.write(); MemorySet::from_copy_on_write(\u0026amp;mut write) } } 虽然对于当时的我们来说算是雪中送炭。但这实际上并非合理的设计。\n第一阶段后 —— 分析 FAT32，改造簇链 在第一阶段结束后，我们通过追踪读写相关代码所耗费的时间, 比如：\n// 当前 pub fn write_at(\u0026amp;self, offset: usize, buf: \u0026amp;[u8]) -\u0026gt; usize { #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] time_trace!(\u0026quot;write_at\u0026quot;); ... } pub fn read_at(\u0026amp;self, offset: usize, buf: \u0026amp;mut [u8]) -\u0026gt; usize { #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] time_trace!(\u0026quot;read_at\u0026quot;); } #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] start_trace!(\u0026quot;cluster\u0026quot;); let pre_cluster_cnt = offset / cluster_size; #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] start_trace!(\u0026quot;clone\u0026quot;); let clus_chain = self.cluster_chain.read(); #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] end_trace!(); let mut cluster_iter = clus_chain.cluster_vec.iter().skip(pre_cluster_cnt); #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] end_trace!(); (TimeTracer 用于记录执行到当前代码的时的系统时间, 当 Timetracer drop 时统计期间消耗的时间)\n首先定位到影响 FAT32 文件直接读写文件过程中最为耗时的操作：遍历文件的簇链过程中，不断在 FAT 表中查询下一簇的位置。我们实现的 FAT32 文件系统所有读写，包括遍历簇链的操作都是依赖 BlockCache 提供的 get_block_cache 方法，但其中遍历簇链这个操作非常的频繁，导致整个读写过程耗时严重。\n读写文件时：\n// 第一阶段 submit 分支 while index \u0026lt; end { let cluster_offset_in_disk = self.fs.read().bpb.offset(curr_cluster); let start_block_id = cluster_offset_in_disk / BLOCK_SIZE; for block_id in start_block_id..start_block_id + spc { ... } if index \u0026gt;= end { break; } curr_cluster = self.fs.read() .fat.read() .get_next_cluster(curr_cluster).unwrap(); curr_cluster = clus_chain.current_cluster; } // 第一阶段 submit 分支 pub fn get_next_cluster(\u0026amp;self, cluster: u32) -\u0026gt; Option\u0026lt;u32\u0026gt; { let (block_id, offset_in_block) = self.cluster_id_pos(cluster); let next_cluster: u32 = get_block_cache(block_id, Arc::clone(\u0026amp;self.device)) .read() .read(offset_in_block, |\u0026amp;next_cluster: \u0026amp;u32| next_cluster); assert!(next_cluster \u0026gt;= 2); if next_cluster \u0026gt;= END_OF_CLUSTER { None } else { Some(next_cluster) } } 基本流程为：IMAGE\n由于该操作对于同一文件的读写非常频繁，并且大部分情况下，特别是多次读文件时，所获取的簇链信息不变，但每次都要重新从 BlockCache 中读取，显然这个过程是可以优化的。一个非常容易想到的方法就是：\n在打开文件时，预读簇链，将簇链信息保存下来 对于可能会修改簇链的文件写操作，如在文件 increase_size 时，重新读取簇链 于是我们对 FAT32 文件系统的簇链设计做出修改：\n// 当前 pub struct ClusterChain { ... pub(crate) cluster_vec: Vec\u0026lt;u32\u0026gt;, } // 当前 // open / increase_size 时 loop { ... let next_cluster = get_block_cache(block_id, Arc::clone(\u0026amp;self.device)) .read() .read(offset_left, |\u0026amp;cluster: \u0026amp;u32| cluster); if next_cluster \u0026gt;= END_OF_CLUSTER { break; } else { self.cluster_vec.push(next_cluster); }; } 对比视频：IMAGE\n第一阶段后 —— 更进一步，Page Cache 其实经过以上优化，内核运行测试时的速度已经能够接受了。但是问题的关键还是在于上面提到的：由于只完成的 FAT32 文件系统，内核中不存在虚拟文件系统这一抽象，也没有 Inode 层面的缓存，内核中的文件实际上将 FAT32 提供的 VirtFile 进一步封装成 KFile，这导致每次写入数据都会同步到磁盘。\n文件读写时，大量的磁盘直接 IO，才是急需解决的问题。由于测试过程中创建的文件的读写操作实际上是在内存中通过 Page Cache 进行的，往往不会直接写回文件系统，特别是在单核下，大量的对磁盘的直接读写会导致内核执行速度变慢（实际我们在多核下测试开启写回操作对速度影响不大）。\n我们首先想到了可以通过实现 TempFS (将文件存储在系统的内存中而不是磁盘上)，将测试过程中内核关于文件的操作如创建，读写等，都交给 TempFS，不必每次都写回 FAT32。或者是在目前的 KFile 基础上加上 PageCache 减少对磁盘的频繁访问。\n由于缺乏相关实现经验，查看了部分第一阶段提交时的优秀队伍的是否有做相关工作，希望从中学到实现思路。\n最后我们选择参考同一期的优秀队伍 TitanixOS 的 PageCache 实现思路，为内核加入 PageCache 机制。\n相关结构如下：\npub struct KFile { // read only feilds readable: bool, writable: bool, path: AbsolutePath, name: String, // shared by some files (uaually happens when fork) pub time_info: Mutex\u0026lt;InodeTime\u0026gt;, pub offset: Mutex\u0026lt;usize\u0026gt;, pub flags: Mutex\u0026lt;OpenFlags\u0026gt;, pub available: Mutex\u0026lt;bool\u0026gt;, // shared by the same file (with page cache) pub inode: Arc\u0026lt;Inode\u0026gt;, } pub struct Inode { pub file: Mutex\u0026lt;Arc\u0026lt;VirtFile\u0026gt;\u0026gt;, fid: u64, #[cfg(not(feature = \u0026quot;no-page-cache\u0026quot;))] pub page_cache: Mutex\u0026lt;Option\u0026lt;Arc\u0026lt;PageCache\u0026gt;\u0026gt;\u0026gt;, #[cfg(not(feature = \u0026quot;no-page-cache\u0026quot;))] pub file_size: Mutex\u0026lt;usize\u0026gt;, } 关于 Inode 的设计：\n配合 InodeCache 加快查找效率，实现文件的缓存 file 字段为 FAT32 提供的 VirtFile 关于在 Inode 中保存文件的大小的理由: 测试过程中创建的文件的读写操作实际上是在内存中通过 Page Cache 进行的，往往不会直接写回文件系统。为了提高测试速度，我们使用 Rust Feature 机制，默认关闭 Inode Drop 时的写回操作，相当于借助 FAT32 VirtFile 的 ”外壳“ 实现了一个不算标准的 TempFS； 文件读写过程中需要用到 file_size 参数，加上不会每次写文件或关闭文件后直接写回文件系统， 故不能通过在文件系统中读取文件大小的方式来获取文件大小（不一致）； 不同进程对该文件进行写操作时会改写文件的大小，使用 Inode Cache 再次打开文件时必须保证文件大小的一致性。 pub struct PageCache { inode: Option\u0026lt;Weak\u0026lt;VirtFile\u0026gt;\u0026gt;, // page number -\u0026gt; page pub pages: RwLock\u0026lt;BTreeMap\u0026lt;usize, Arc\u0026lt;FilePage\u0026gt;\u0026gt;\u0026gt;, } pub struct FilePage { pub permission: MapPermission, pub data_frame: FrameTracker, pub file_info: Option\u0026lt;Mutex\u0026lt;FilePageInfo\u0026gt;\u0026gt;, } pub struct FilePageInfo { /// 页的起始位置在文件中的偏移（一定是页对齐的） pub file_offset: usize, pub data_states: [DataState; PAGE_SIZE / BLOCK_SIZE], inode: Weak\u0026lt;VirtFile\u0026gt;, } 文件操作图片：IMG\n对比图片：IMG\n至此，我们的内核文件优化过程暂时告一段落。\n","date":"2023-08-17","permalink":"https://bitethedisk.github.io/post/fs-optimization/","tags":["优化"],"title":"文件系统优化历程"},{"content":"实现动态连接十分简单，只需要在加载elf文件时判断是否存在interpret段，如果存在就加载对应的动态连接器， 将动态连接器内容映射进内存并将程序入口设置为动态连接器的入口。接下来动态连接器会完成剩下的连接工作。\n尽管原理很简单，但由于动态连接涉及的系统调用较多，在加入动态连接后很容易出bug。 未正确处理mmap，没压入或压入了错误的aux数组，fstat返回空结构体都可能导致动态连接失败。 值得一提的是，这里使用的动态连接器是musl libc，它既是动态连接器又是函数库， 测例中使用的函数以及动态连接的过程都可以从这个库的代码中找到。阅读这部分代码会对调试程序有帮助。\n","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/dynamic/","tags":["全国赛第一阶段"],"title":"动态链接"},{"content":"在 Rust 中，字符串是一个胖指针，包括具有所有权的 String 以及 \u0026amp;'a str，并且为 utf-8 编码\n而在 C 中，字符串只是一个单纯的指针，以有符号的 i8 为单元，以 \\0 作为结尾，长度不包括结尾 \u0026lsquo;\\0\u0026rsquo;\n在使用 Rust 编写 initrpoc 和 系统调用的时候，由于测例所要求的的接口所使用的的字符串相关参数均为 C 标准，所以我们需要将 Rust 和 C 之间的字符串进行转换\nCString Rust 中提供了 alloc::ffi:CString 来处理两者之间的转换\n创建 CString 一般可以通过 CString::new 来创建一个 CString\npub fn new\u0026lt;T: Into\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt;(t: T) -\u0026gt; Result\u0026lt;CString, NulError\u0026gt; 具体如下:\n#[macro_use] extern crate alloc; use alloc::ffi::CString; fn main() { let c_string = CString::new(\u0026quot;Hello, world!\u0026quot;).unwrap(); // ... } 这里的 new 方法会检查传入的值是否是以 \\0 结尾的，该方法本身会将传入的值自动加上 \\0，所以要求传入的参数所表示的字符串不能以 \\0 结尾\n也可以通过 unsafe 方法 CString::from_vec_unchecked 等忽略这种检查\nCString 转换需注意 Rust 是一门强调安全的语言，所有的值都需要在超出各自的声明周期后释放其所占有的内存\n文档中提供了一个 pub fn into_raw(self) -\u0026gt; *mut c_char 方法，该方法的实际意义和它的签名并不相同: 其返回的字符串必须不可变！\n因为从 Rust 向 C 传递的字符串本质上还是当前 Rust 程序分配的内存(在堆上，本质还是一个 Vec\u0026lt;u8\u0026gt;，对，是 u8，而不是 i8，估计是为了防止由于溢出而导致的编译失败，rustc dddd)，如果在 C 程序中改变了当前字符串的值，比如删去结尾 \\0，或者将 \\0 提前，都会带来内存安全隐患\n所以，这里的 *mut c_char 实际对应 C 中的 char *\n及时回收 CString 防止内存泄露 前面提到，CString 底层其实是 Vec\u0026lt;u8\u0026gt;，其实际上是分配在 Rust 的堆上的\n在将 CString leek 成传给 C 的指针后，CString 底层的 Vec\u0026lt;u8\u0026gt; 就脱离了 Rust 所有权机制的监管，如果不重构裸指针的所有权，那将会导致内存泄露\n要想回收 CString 所使用的内存，可以按以下操作\n#[macro_use] extern crate alloc; use alloc::ffi::CString; fn main() { let initproc = CString::new(\u0026quot;/busybox\u0026quot;).unwrap(); let initproc_raw = initproc.into_raw(); // ... // Execute syscall, which uses C ABI. execve(iniproc, /* argv, envs */); // Release memory of `initproc_raw`. { unsafe { CString::from_raw(initproc_raw); } } } ","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/rust-and-c/","tags":["全国赛第一阶段","FFI"],"title":"Rust 和 C 的交互"},{"content":"在目前的实现中信号总共有63种，1-31为非实时信号，34-63是实时信号。32和33为未定义信号。 信号是每个进程独有的，除此之外每个进程还有信号掩码。\n涉及信号处理的共有3个系统调用：SYS_SIGACTION，SYS_SIGPROCMASK，SYS_SIGRETURN。\n涉及的系统调用 sys_sigaction用于为一个信号注册信号处理函数，当进程接受到信号后会跳转到这个信号处理函数。 一般信号处理函数后会调用sigreturn将程序上下文恢复到执行信号处理函数之前的状态。 但程序也可能不调用sigreturn，而是使用longjump跳转到别的位置，内核不关心信号处理函数是否返回， 不在内核中维护信号处理上下文信息，而是在执行信号处理函数之前将上下文压入用户栈，并在sigreturn时从用户栈中恢复信息。 在执行信号处理函数的过程中，这些保存的上下文信息可能被用户程序修改，用户程序可以借此返回到不同的地方。\nsys_sigprocmask用于修改进程的信号掩码。信号掩码可以用来屏蔽信号，被屏蔽的信号会被阻塞， 直到信号掩码不再阻塞该信号时该信号才会被处理。 9号信号和19号信号不能被阻塞。 sys_sigreturn会从用户栈中取出上下文信息将程序恢复到信号处理前的状态。 sys_sigreturn没有返回值，在执行完后不应将执行结果写入a0寄存器。\n由于执行信号处理函数时与执行其他用户函数没有区别，linux的信号设计自然支持信号处理的嵌套， 只要正确实现了这几个系统调用，无需在内核内保存额外信息就可以支持信号的嵌套调用。 一般信号的产生是通过sys_kill等系统调用产生的，除此之外进程退出时也会产生信号。 信号处理的时机是不确定的。现在内核中会在每次返回用户态之前检查有无需要处理的信号。\n与其他系统调用的交互 sys_wait4 、sys_read、sys_futex等具有阻塞等待行为的系统调用可以被信号中断， 如果在阻塞过程中有到来的信号应该停止等待，返回被中断的错误码。 fork 出来的子进程应该继承父进程的注册的信号处理程序，和信号掩码。 exec 后程序应该清空信号处理程序但是保留信号掩码。\n","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/signal/","tags":["全国赛第一阶段"],"title":"[全国赛I] 信号处理"},{"content":"由于大部分测试需要使用 busybox，为了避免多次解析 elf、从零创建地址空间等问题，我们采用了类似于加载initproc的方法。\n具体而言，我们将 busybox 预加载到内核中，并保存 load_elf 获取的信息。每次执行busybox时，我们直接使用保存的 load_elf 信息，并通过写时拷贝来创建所需的 busybox 进程的地址空间，更快速地创建 busybox 进程从而实现更高效的测试。\n// kernel/src/task/initproc/mod.rs pub static ref BUSYBOX: RwLock\u0026lt;Busybox\u0026gt; = RwLock::new({ extern \u0026quot;C\u0026quot; { fn busybox_entry(); fn busybox_tail(); } let entry = busybox_entry as usize; let tail = busybox_tail as usize; let siz = tail - entry; let busybox = unsafe { core::slice::from_raw_parts(entry as *const u8, siz) }; let path = AbsolutePath::from_str(\u0026quot;/busybox0\u0026quot;); let inode = fs::open(path, OpenFlags::O_CREATE, CreateMode::empty()).expect(\u0026quot;busybox0 create failed\u0026quot;); inode.write_all(\u0026amp;busybox.to_owned()); let bb = Arc::new(TaskControlBlock::new(inode.clone())); inode.delete(); Busybox { inner: bb, } }); pub static mut ONCE_BB_ENTRY: usize = 0; pub static mut ONCE_BB_AUX: Vec\u0026lt;AuxEntry\u0026gt; = Vec::new(); pub struct Busybox { inner: Arc\u0026lt;TaskControlBlock\u0026gt;, } impl Busybox { pub fn elf_entry_point(\u0026amp;self) -\u0026gt; usize { unsafe { ONCE_BB_ENTRY } } pub fn aux(\u0026amp;self) -\u0026gt; Vec\u0026lt;AuxEntry\u0026gt; { unsafe { ONCE_BB_AUX.clone() } } pub fn memory_set(\u0026amp;self) -\u0026gt; MemorySet { let mut write = self.inner.memory_set.write(); MemorySet::from_copy_on_write(\u0026amp;mut write) } } ","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/busybox-pre-init/","tags":["全国赛第一阶段"],"title":"[全国赛I] busybox 预加载"},{"content":"在我们的内核中，我们使用 TASKMANAGER 管理分别处于就绪态，阻塞态的进程，包括因为调用 nanosleep 而休眠的进程。\n// kernel/src/task/manager.rs // 负责管理待调度的进程对象 pub struct TaskManager { ready_queue: VecDeque\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, waiting_queue: VecDeque\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, hq: BinaryHeap\u0026lt;HangingTask\u0026gt;, } // 用于管理 sleep 进程 pub struct HangingTask { wake_up_time: usize, // ns inner: Arc\u0026lt;TaskControlBlock\u0026gt;, } // 用于处理子线程的资源回收 pub struct ChildrenThreadMonitor { cancelled_child_threads: Vec\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, } // 维护内核中 pid 到 TCB 的映射 pub static ref PID2TCB: Mutex\u0026lt;BTreeMap\u0026lt;usize, Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;\u0026gt; = Mutex::new(BTreeMap::new()); // 子线程回收管理器 pub static CHILDREN_THREAD_MONITOR: Mutex\u0026lt;ChildrenThreadMonitor\u0026gt; = Mutex::new(ChildrenThreadMonitor::new()); // kernel/src/task/processor/processor.rs /// [`Processor`] 是描述 CPU执行状态 的数据结构。 /// 在单核环境下，我们仅创建单个 Processor 的全局实例 PROCESSOR pub static mut PROCESSOR: SyncRefCell\u0026lt;Processor\u0026gt; = SyncRefCell::new(Processor::new()); /// 每个核上的处理器，负责运行一个进程 pub struct Processor { /// 当前处理器上正在执行的任务 current: Option\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, /// 当前处理器上的 idle 控制流的任务上下文 idle_task_cx: TaskContext, } run_tasks 分别尝试从 hang_task, ready_task 队列中获取进程调度。\n// kernel/src/task/processor/schedule.rs /// 进入 idle 控制流，它运行在这个 CPU 核的启动栈上， /// 功能是循环调用 fetch_task 直到顺利从任务管理器中取出一个任务，随后便准备通过任务切换的方式来执行 pub fn run_tasks() { let bb = BUSYBOX.read(); // lazy static busybox drop(bb); loop { let processor = acquire_processor(); recycle_child_threads_res(); if let Some(hanging_task) = check_hanging() { run_task(hanging_task, processor); } else if let Some(interupt_task) = check_futex_interupt_or_expire() { unblock_task(interupt_task); } else if let Some(task) = fetch_task() { run_task(task, processor); } } } 值得一提的是，我们在进程调度时还需要检测 block_task 队列中， 因为在系统调用过程中被信号打断的 task 是否有处理完信号，或者 futex_wait 时给出的 timeout 是否已超时以唤醒该进程并加入到 ready_task 中。\n// kernel/src/task/manager.rs pub fn check_futex_interupt_or_expire(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt; { for tcb in self.waiting_queue.iter() { let lock = tcb.inner_ref(); // 被信号打断的 task 是否有处理完信号 if !lock.pending_signals.difference(lock.sigmask).is_empty() { return Some(tcb.clone()); } } let mut global_futex_que = FUTEX_QUEUE.write(); for (_, futex_queue) in global_futex_que.iter_mut() { // timeout 是否已超时 if let Some(task) = futex_queue.pop_expire_waiter() { return Some(task.clone()); } } None } ","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/scheduling/","tags":["进程","线程","全国赛第一阶段"],"title":"[全国赛I] 进程/线程调度"},{"content":"TCB 与 fork 的更改 相较于初赛，完善了 clone 系统调用。初赛时由于要求测试样例要求比较低，在实现 clone 系统调用时并未完全利用用户传递的参数。我们根据 Linux manual page 中的要求，完善了内核的 fork 以及 TaskControlBlock 结构。\n// kernel/task/task.rs pub struct TaskControlBlock { ... pub sigactions: Arc\u0026lt;RwLock\u0026lt;[SigAction; MAX_SIGNUM as usize]\u0026gt;\u0026gt;, pub memory_set: Arc\u0026lt;RwLock\u0026lt;MemorySet\u0026gt;\u0026gt;, pub fd_table: Arc\u0026lt;RwLock\u0026lt;FDTable\u0026gt;\u0026gt;, pub robust_list: Arc\u0026lt;RwLock\u0026lt;RobustList\u0026gt;\u0026gt;, pub rlimit_nofile: Arc\u0026lt;RwLock\u0026lt;RLimit\u0026gt;\u0026gt;, inner: RwLock\u0026lt;TaskControlBlockInner\u0026gt;, } pub struct TaskControlBlockInner { ... pub pending_signals: SigSet, pub sigmask: SigMask, pub interval_timer: Option\u0026lt;IntervalTimer\u0026gt;, pub utime: TimeVal, pub stime: TimeVal, pub last_enter_umode_time: TimeVal, pub last_enter_smode_time: TimeVal, pub clear_child_tid: usize, /* CLONE_CHILD_CLEARTID */ } 相较于初赛，我们为 TCB 加入了有关信号、时间、资源等结构。并根据 sys_clone 传递的参数，正确地实现 fork，比如以下代码段：\n// kernel/src/task/task.rs(fn fork) // 拷贝用户地址空间 let memory_set = if flags.contains(CloneFlags::VM) { self.memory_set.clone() } else { Arc::new(RwLock::new(MemorySet::from_copy_on_write( \u0026amp;mut self.memory_set.write(), ))) }; if flags.contains(CloneFlags::THREAD) { memory_set.write().map_thread_trap_context(private_tid); } // kernel/syscall/impls/process.rs (sys_do_fork) if flags.contains(CloneFlags::PARENT_SETTID) { *translated_mut(current_user_token(), ptid as *mut u32) = new_pid as u32; } if flags.contains(CloneFlags::CHILD_SETTID) { *translated_mut(child_token, ctid as *mut u32) = new_pid as u32; } if flags.contains(CloneFlags::CHILD_CLEARTID) { new_task.inner_mut().clear_child_tid = ctid; } if flags.contains(CloneFlags::SETTLS) { let trap_cx = new_task.inner_mut().trap_context(); trap_cx.set_tp(tls); } 线程的引入 在 fork 过程中，当 CloneFlags 中存在 CLONE_THREAD 位时，正在创建的进程当前进程的为子线程\nPid 分配器的更改\n由于我们的内核在实现线程时，为了更方便地为子线程分配 TrapContext Frame 资源，我们规定子线程的 tid (pid) 不应该与小于进程（主线程）的 tid (pid) ，故移除了进程 pid 的回收操作。\n子进程与子线程的区分\n目前我们将子线程与子进程均保存在 TCB 的 children 字段，在遇到进程退出等问题时会判断 child 是子进程还是子线程。\n目前通过 tgid 与 pid 来区分 TCB 是父进程的子进程还是子线程\n// kernel/src/task/task.rs(fn fork) let pid_handle = pid_alloc(); let tgid = if flags.contains(CloneFlags::THREAD) { self.pid.0 } else { pid_handle.0 }; 如果 tgid 与 pid 值相同，则该 TCB 为进程，否则为线程\n为线程分配资源\n线程除了共享主线程（进程）的 memory_set, fd_table, sigaction 等资源，还需要一些独立的资源如ID, 内核地址空间的KernelStack，以及主线程中独立分配的 TrapContext Frame:\n// kernel/syscall/impls/process.rs (sys_do_fork) if flags.contains(CloneFlags::THREAD) { memory_set.write().map_thread_trap_context(private_tid); } // kernel/src/task/ttask.rs pub fn trap_context_position(tid: usize) -\u0026gt; VirtAddr { VirtAddr::from(TRAP_CONTEXT - tid * PAGE_SIZE) } 其中，private_tid 为tgid(主线程/父进程)与pid(子线程tid)的差值\n进程/线程的退出 当前进程结束的方式包括：\n进程运行完代码段，非法访问到 .rodata 引发 trap，在 trap 处理中回收进程对象 进程调用 exit 系统调用 进程收到 kill 相关的信号，在信号处理时退出 关于进程/线程的退出时需要做的工作包括：\n完成进程的初步回收： 将自身从 PID2TCB 映射管理器中移除 标记自身状态为 Zombie，记录退出码 将子进程移交给 initproc 如果自身为某进程的子线程，还需要找到主进程并将自身从主进程中移除，并压入子线程回收管理器 CHILDREN_THREAD_MONITOR 中，在下一次进程调度时回收可回收的资源 进程的父进程等待子进程退出，调用 wait 系统调用，完成子进程资源的回收：找到子进程中处于 Zombie 态的进程并且强引用计数为 1 的进程，移除该进程以彻底回收该进程的所有资源 子线程资源回收\n子线程退出时，子线程会加入到回收管理器 CHILDREN_THREAD_MONITOR 中，并在下一次进程调度时回收可回收的资源。\n// kernel/src/task/mod.rs (fn exit_current_and_run_next) if is_child_thread { let parent = inner.parent.as_ref().unwrap().upgrade().unwrap(); let mut parent_inner = parent.inner_mut(); let children_iter = parent_inner.children.iter(); let (idx, _) = children_iter .enumerate() .find(|(_, t)| t.pid() == pid) .unwrap(); parent_inner.children.remove(idx); drop(parent_inner); drop(parent); drop(inner); assert!(Arc::strong_count(\u0026amp;task) == 1); take_cancelled_chiled_thread(task); schedule(\u0026amp;mut TaskContext::empty() as *mut _); unreachable!() } 这个过程本身其实可以不用做，而是等主线程进行 wait 系统调用时彻底回收。 但由于测试过程中，进程会创建成千上万个子线程，如果这些线程资源没有及时回收，如 TrapContext, KenerlStack 等资源，会浪费许多内存资源。\n其实 take_cancelled_chiled_thread(task)这段代码本身，以及 CHILDREN_THREAD_MONITOR变量，也就是说这段代码本身其实可以直接改为 drop(task)， 因为此时 task 强引用计数一定为 1，task 中可以释放的资源都可以在, schedule 之前释放掉但是 task 在执行 exit_current_and_run_next时本身出于内核态，此时回收 task 的 KerenlStack 既不符合逻辑，又有可能产生一些隐患，故选择使用 CHILDREN_THREAD_MONITOR 在调度时释放退出的线程。\n","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/process-thread/","tags":["进程","线程","全国赛第一阶段"],"title":"[全国赛I] 进程管理-进程与线程"},{"content":"一开始\n","date":"2023-07-12","permalink":"https://bitethedisk.github.io/post/opensbi-getchar/","tags":["全国赛第一阶段"],"title":"适配 OpenSBI"},{"content":"Rust 和 C 交互的时候需要注意 Rust 的堆，当从 Rust 程序 fork 出一个进程 来执行 C 程序的时候，C 是不能识别 Rust 的堆的(包括其堆管理器这些)，所以当 使用 C 程序去访问 fork 后的 Rust 堆上的对象，或者传递相关堆上对象的时候可能 会产生致命问题(如非法内存访问\u0026mdash;访问越界)\n","date":"2023-07-07","permalink":"https://bitethedisk.github.io/post/rust-heap-and-c/","tags":null,"title":"Rust Heap and C"},{"content":"随着开发的进行，我们需要的适配和封装的数据结构越来越多，其中大部分与我们的内核本体关系并没有那么紧密， 所以我们将这部分结构，如用于的引导程序、FAT32、Linux 相关数据结构放在了项目根目录中的 crates 里\nRust 本身是支持多个 crates 构成的一个 workspace，这些 crates 直接可以相互引用，但是由于我们使用 了 .cargo/config.toml 来配置 rustc，所以 workspace 并不能为我们所有 (因为目前 workspace 不支持在 workspace 中读取 .cargo/config.toml)\n使用 Git Submodule 管理测例 与区域赛不同，全国赛的测例数目较多，如果一旦发生更新构建起来也相对麻烦\n基于 Git Submodule 我们可以方便隔离当前 Git 仓库，做到依赖的隔离与同步\n就当前的实际环境来说:\ngit submodule add https://github.com/oscomp/testsuits-for-oskernel.git testsuits 上面的作用是将 testsuits-for-oskernel.git clone 到本地的 testsuits 文件夹中，后者会自动创建\n当重新拉取项目仓库时:\ngit submodule init git submodule update 就可以重新拉取 testsuits 中，仓库的内容了\n项目目录树 . ├── Makefile ├── README.md ├── crates │ ├── fat32/ ---- FAT32 读写库 │ ├── libd/ ---- libc 的~~后继者(划掉)~~ initproc，内核自动加载的第一个用户程序 │ ├── nix/ ---- Linux 相关数据结构 │ └── sync_cell/ ---- 实现了 Sync 的，具有内部可变性的 RefCell ├── docs/ ├── kernel/ │ ├── Makefile │ ├── build.rs ---- 用于监控相关文件，如 `crates/libd/bin/initproc.rs`，发生变化时重新编译 │ ├── cargo │ │ └── config.toml │ ├── linkerld │ │ └── linker.ld │ ├── src │ │ ├── boards │ │ │ └── qemu.rs ---- 平台相关参数 │ │ ├── console.rs │ │ ├── consts.rs │ │ ├── drivers │ │ ├── entry.S │ │ ├── error.rs │ │ ├── fs/ │ │ ├── logging.rs │ │ ├── macros │ │ │ ├── hsm.rs │ │ │ ├── mod.rs │ │ │ ├── on_boot.rs │ │ │ └── profile.rs ---- 用于打印某段代码运行时间的宏 │ │ ├── main.rs │ │ ├── mm │ │ ├── sbi.rs │ │ ├── syscall/ │ │ ├── task/ │ │ ├── timer.rs │ │ └── trap/ │ ├── target/ ---- 构建产物 │ └── vendor/ ---- 所有第三方依赖的本地归档 ├── testsuits/ ---- 通过 Git Submodule 内联的官方测例 └── workspace ---- 用于中间过程构建内核运行所需测例 1187 directories, 9162 files ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/project-structure/","tags":["全国赛第一阶段"],"title":"[全国赛I] 项目结构(rust-workspace不能使用)"},{"content":"项目提供了一系列的 Makefile 来简化开发流程.\n一般只会用到项目根目录中的 Makefile:\nBOOTLOADER_ELF = ./kernel/bootloader/rustsbi-qemu KERNEL_ELF = ./kernel/target/riscv64gc-unknown-none-elf/release/kernel sbi-qemu: @cp $(BOOTLOADER_ELF) sbi-qemu kernel-qemu: @mv kernel/cargo kernel/.cargo @cd kernel/ \u0026amp;\u0026amp; make kernel @cp $(KERNEL_ELF) kernel-qemu all: sbi-qemu kernel-qemu clean: @rm -f kernel-qemu @rm -f sbi-qemu @rm -rf build/ @rm -rf temp/ @cd kernel/ \u0026amp;\u0026amp; cargo clean @cd workspace/ \u0026amp;\u0026amp; make clean @cd fat32/ \u0026amp;\u0026amp; cargo clean @cd misc/ \u0026amp;\u0026amp; make clean fat32img: @cd kernel/ \u0026amp;\u0026amp; make fat32img run: @cd kernel/ \u0026amp;\u0026amp; make run debug-server: @cd kernel/ \u0026amp;\u0026amp; make debug-server debug: @cd kernel/ \u0026amp;\u0026amp; make debug 这里只需关注以下几点:\nrun: 构建内核并运行，内核是以 release 方式构建的 debug-server: 以 debug 方式构建内核并运行 gdb debug server debug: 链接上面运行的 debug server 开始调试 ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/about-makefile/","tags":["Makefile","Debug","区域赛"],"title":"[区域赛] 有关 BTD 的 Makefile"},{"content":"在 BTD 的开发中经常会遇到 debug 的需求，BTD 提供了一个方便的调试流程，只需启动两个 shell 即可 开始调试\n下面将演示一次具体的调试流程\n启动两个 shell，在项目根目录下分别运行 make debug-server 和 make debug， 之后只需要在运行 make debug 的 shell 中执行接下来的命令即可 # 从当前 pc 开始显示 11 条指令，11 条是不发生折叠的极限（也可能只是我这是这样） (gdb) x/11i $pc (gdb) b *0x80000000 # 在内核第一条指令处打个断点 (gdb) c # continue 执行到断点处 (gdb) si # si 单步执行 (step in，会嵌入函数具体流程中) 需要注意的是，在多核运行时，代码执行过程中会出现系统线程的切换，例如下面这样：\n(gdb) c Continuing. [Switching to Thread 1.2] 这时候我们要看当前线程的状态，根据当前状态进行调试，该打断点的地方不要忘了打断点，不然会跑飞\n像是这样，只能用 Ctrl + C 来掐死 (主要是我也没去找其他可能的方法 😛):\n看到地址了吗，直接归零，而且这块地址是不可访问的，只能卡在这了，除非扬了当前的 shell 重来（大概\nThread 2 received signal SIGINT, Interrupt. 0x0000000000000000 in ?? () (gdb) x/11i $pc =\u0026gt; 0x0: Cannot access memory at address 0x0 (gdb) ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/how-to-debug-the-kernel/","tags":["Debug"],"title":"如何调试 BTD-OS"},{"content":"内核在运行的时候总是会不知何时卡死，底层原因是持续触发时钟中断\n// 时间片到了 Trap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { set_next_trigger(); // 主要是这里 suspend_current_and_run_next(); } 初步解决 我们将设置下一个时钟中断放置在了 suspend_current_and_run_next 之前，导致可能因为后者执行 时间过长而使用户态一直处于时钟中断触发状态，至于为什么会在 RV64 上一直触发中断，可以参阅 RV 的特权级手册:\nPlatforms provide a real-time counter, exposed as a memory-mapped machine-mode read-write register, mtime. mtime must increment at constant frequency, and the platform must provide a mechanism for determining the period of an mtime tick. The mtime register will wrap around if the count overflows.\nThe mtime register has a 64-bit precision on all RV32 and RV64 systems. Platforms provide a 64- bit memory-mapped machine-mode timer compare register (mtimecmp). A machine timer interrupt becomes pending whenever mtime contains a value greater than or equal to mtimecmp, treating the values as unsigned integers. The interrupt remains posted until mtimecmp becomes greater than mtime (typically as a result of writing mtimecmp). The interrupt will only be taken if interrupts are enabled and the MTIE bit is set in the mie register.\n由于 suspend_current_and_run_next 执行的时间超过了一个时间片的长度，导致其返回用户态进程时， mtime 的值已经大于了 set_next_trigger 设置的时间点，由上文可得，如果 mtime 大于等于 mtimecmp(即 set_next_trigger 设置的值)，并且 mie 为使能状态，那么时钟中断会一直处于触发状态.\n而我们的内核 mie 一直处于使能状态，所以 S 态的时钟中断会持续在用户态发生(S 态中断不会打断同级与 更高特权级代码的执行)，导致用户态毫无进展，而我们内核的引导程序 initproc 会一直等待卡死用户进程 变为僵尸态，所以造成了内核执行流的卡死.\n解决办法:\n简单调整下位置\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); set_next_trigger(); } 但这样真的对吗? 不对，因为会导致用户态程序卡死整个内核的执行流\n一个致命的缺点是，用户态的程序需要第一次运行后才能正确的获取时钟中断，不然只能等轮回一边后才可能正确让出\n当前的逻辑是:\nRustSBI 完成初始化后，在 meow(没错，这是我们 Rust 代码的 ENTRYPOINT)，中初步设定一个时钟中断\n#[cfg(not(feature = \u0026quot;multi_harts\u0026quot;))] #[no_mangle] pub fn meow() -\u0026gt; ! { if hartid!() == 0 { init_bss(); unsafe { set_fs(FS::Dirty) } lang_items::setup(); logging::init(); mm::init(); trap::init(); trap::enable_stimer_interrupt(); trap::set_next_trigger(); fs::init(); task::add_initproc(); task::run_tasks(); } else { loop {} } unreachable!(\u0026quot;main.rs/meow: you should not be here!\u0026quot;); } 这是第一个问题，我们原本想的是，这个时钟中断会在第一用户态程序运行时发生，但是有可能它在 fs::init() 或者 task::add_initproc() 中已经发生了，这会导致一进入用户态程序就发生中断，这和我们 预期的不一样.\n而且，陷入中断后，除非使失能 mie，或者再次 set_next_trigger()(又或者 mtime 发生回环)， 否则将一直处于中断触发的状态\n而这之后切换的用户进程都会遇到中断而直接返回，直到运行到第一个用户进程(其实应该是引导程序 initproc)， 在下面 suspend_current_and_run_next 真正意义上的返回后，重新设置下一个中断时间点，这才能让 OS 内核 所有的用户进程进入正常的运行流.\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); set_next_trigger(); } 而这种时间上的开销显然是没必要的，所以我们根据所有用户进程都会通过 trap_return 返回用户态这一点， 将 set_next_trigger 设置在了 trap_return 中，同时判断当前进程是否是因为时间片耗尽而导致的 trap 返回:\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); } // ... #[no_mangle] pub fn trap_return() -\u0026gt; ! { // ... if is_time_intr_trap() { set_next_trigger(); } // ... } /// 是否是由于时间片耗尽导致的 trap fn is_time_intr_trap() -\u0026gt; bool { let scause = scause::read(); scause.cause() == Trap::Interrupt(scause::Interrupt::SupervisorTimer) } 最终结果 我们消除了一个严重的 bug: 内核在执行用户程序时随机卡死 删去了 meow 中不利于系统鲁棒性的代码 ","date":"2023-06-29","permalink":"https://bitethedisk.github.io/post/random-stuck/","tags":["问题与解决"],"title":"[已解决] 内核在执行用户程序时随机卡死"}]