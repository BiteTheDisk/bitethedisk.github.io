<<<<<<< HEAD
[{"content":"","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/rust-and-c/","tags":["全国赛第一阶段","FFI"],"title":"Rust 和 C 的交互"},{"content":"在目前的实现中信号总共有63种，1-31为非实时信号，34-63是实时信号。32和33为未定义信号。 信号是每个进程独有的，除此之外每个进程还有信号掩码。 涉及信号处理的共有3个系统调用：SYS_SIGACTION，SYS_SIGPROCMASK，SYS_SIGRETURN。\n涉及的系统调用 sys_sigaction用于为一个信号注册信号处理函数，当进程接受到信号后会跳转到这个信号处理函数。一般信号处理函数后会调用sigreturn将程序上下文恢复到执行信号处理函数之前的状态。但程序也可能不调用sigreturn，而是使用longjump跳转到别的位置，内核不关心信号处理函数是否返回，不在内核中维护信号处理上下文信息，而是在执行信号处理函数之前将上下文压入用户栈，并在sigreturn时从用户栈中恢复信息。在执行信号处理函数的过程中，这些保存的上下文信息可能被用户程序修改，用户程序可以借此返回到不同的地方。\nsys_sigprocmask用于修改进程的信号掩码。信号掩码可以用来屏蔽信号，被屏蔽的信号会被阻塞，直到信号掩码不再阻塞该信号时该信号才会被处理。 9号信号和19号信号不能被阻塞。 sys_sigreturn会从用户栈中取出上下文信息将程序恢复到信号处理前的状态。 sys_sigreturn没有返回值，在执行完后不应将执行结果写入a0寄存器。\n由于执行信号处理函数时与执行其他用户函数没有区别，linux的信号设计自然支持信号处理的嵌套，只要正确实现了这几个系统调用，无需在内核内保存额外信息就可以支持信号的嵌套调用。 一般信号的产生是通过sys_kill等系统调用产生的，除此之外进程退出时也会产生信号。 信号处理的时机是不确定的。现在内核中会在每次返回用户态之前检查有无需要处理的信号。\n与其他系统调用的交互 sys_wait4 、sys_read、sys_futex等具有阻塞等待行为的系统调用可以被信号中断，如果在阻塞过程中有到来的信号应该停止等待，返回被中断的错误码。 fork 出来的子进程应该继承父进程的注册的信号处理程序，和信号掩码。 exec 后程序应该清空信号处理程序但是保留信号掩码。\n","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/signal/","tags":["全国赛第一阶段"],"title":"信号处理"},{"content":"由于大部分测试需要使用 busybox，为了避免多次解析 elf、从零创建地址空间等问题，我们采用了类似于加载initproc的方法。具体而言，我们将 busybox 预加载到内核中，并保存 load_elf 获取的信息。每次执行busybox时，我们直接使用保存的 load_elf 信息，并通过写时拷贝来创建所需的 busybox 进程的地址空间，更快速地创建 busybox 进程从而实现更高效的测试。\n// kernel/src/task/initproc/mod.rs pub static ref BUSYBOX: RwLock\u0026lt;Busybox\u0026gt; = RwLock::new({ extern \u0026quot;C\u0026quot; { fn busybox_entry(); fn busybox_tail(); } let entry = busybox_entry as usize; let tail = busybox_tail as usize; let siz = tail - entry; let busybox = unsafe { core::slice::from_raw_parts(entry as *const u8, siz) }; let path = AbsolutePath::from_str(\u0026quot;/busybox0\u0026quot;); let inode = fs::open(path, OpenFlags::O_CREATE, CreateMode::empty()).expect(\u0026quot;busybox0 create failed\u0026quot;); inode.write_all(\u0026amp;busybox.to_owned()); let bb = Arc::new(TaskControlBlock::new(inode.clone())); inode.delete(); Busybox { inner: bb, } }); pub static mut ONCE_BB_ENTRY: usize = 0; pub static mut ONCE_BB_AUX: Vec\u0026lt;AuxEntry\u0026gt; = Vec::new(); pub struct Busybox { inner: Arc\u0026lt;TaskControlBlock\u0026gt;, } impl Busybox { pub fn elf_entry_point(\u0026amp;self) -\u0026gt; usize { unsafe { ONCE_BB_ENTRY } } pub fn aux(\u0026amp;self) -\u0026gt; Vec\u0026lt;AuxEntry\u0026gt; { unsafe { ONCE_BB_AUX.clone() } } pub fn memory_set(\u0026amp;self) -\u0026gt; MemorySet { let mut write = self.inner.memory_set.write(); MemorySet::from_copy_on_write(\u0026amp;mut write) } } ","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/busybox-pre-init/","tags":["busybox","全国赛I"],"title":"[全国赛I] busybox 预加载"},{"content":"在我们的内核中，我们使用 TASKMANAGER 管理分别处于就绪态，阻塞态的进程，包括因为调用 nanosleep 而休眠的进程。\n// kernel/src/task/manager.rs // 负责管理待调度的进程对象 pub struct TaskManager { ready_queue: VecDeque\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, waiting_queue: VecDeque\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, hq: BinaryHeap\u0026lt;HangingTask\u0026gt;, } // 用于管理 sleep 进程 pub struct HangingTask { wake_up_time: usize, // ns inner: Arc\u0026lt;TaskControlBlock\u0026gt;, } // 用于处理子线程的资源回收 pub struct ChildrenThreadMonitor { cancelled_child_threads: Vec\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, } // 维护内核中 pid 到 TCB 的映射 pub static ref PID2TCB: Mutex\u0026lt;BTreeMap\u0026lt;usize, Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;\u0026gt; = Mutex::new(BTreeMap::new()); // 子线程回收管理器 pub static CHILDREN_THREAD_MONITOR: Mutex\u0026lt;ChildrenThreadMonitor\u0026gt; = Mutex::new(ChildrenThreadMonitor::new()); // kernel/src/task/processor/processor.rs /// [`Processor`] 是描述 CPU执行状态 的数据结构。 /// 在单核环境下，我们仅创建单个 Processor 的全局实例 PROCESSOR pub static mut PROCESSOR: SyncRefCell\u0026lt;Processor\u0026gt; = SyncRefCell::new(Processor::new()); /// 每个核上的处理器，负责运行一个进程 pub struct Processor { /// 当前处理器上正在执行的任务 current: Option\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, /// 当前处理器上的 idle 控制流的任务上下文 idle_task_cx: TaskContext, } run_tasks 分别尝试从 hang_task, ready_task 队列中获取进程调度。\n// kernel/src/task/processor/schedule.rs /// 进入 idle 控制流，它运行在这个 CPU 核的启动栈上， /// 功能是循环调用 fetch_task 直到顺利从任务管理器中取出一个任务，随后便准备通过任务切换的方式来执行 pub fn run_tasks() { let bb = BUSYBOX.read(); // lazy static busybox drop(bb); loop { let processor = acquire_processor(); recycle_child_threads_res(); if let Some(hanging_task) = check_hanging() { run_task(hanging_task, processor); } else if let Some(interupt_task) = check_futex_interupt_or_expire() { unblock_task(interupt_task); } else if let Some(task) = fetch_task() { run_task(task, processor); } } } 值得一提的是，我们在进程调度时还需要检测 block_task 队列中，因为在系统调用过程中被信号打断的 task 是否有处理完信号，或者 futex_wait 时给出的 timeout 是否已超时以唤醒该进程并加入到 ready_task 中。\n// kernel/src/task/manager.rs pub fn check_futex_interupt_or_expire(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt; { for tcb in self.waiting_queue.iter() { let lock = tcb.inner_ref(); // 被信号打断的 task 是否有处理完信号 if !lock.pending_signals.difference(lock.sigmask).is_empty() { return Some(tcb.clone()); } } let mut global_futex_que = FUTEX_QUEUE.write(); for (_, futex_queue) in global_futex_que.iter_mut() { // timeout 是否已超时 if let Some(task) = futex_queue.pop_expire_waiter() { return Some(task.clone()); } } None } ","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/scheduling/","tags":["process","thread","全国赛I"],"title":"[全国赛I] 进程/线程调度"},{"content":"TCB 与 fork 的更改 相较于初赛，完善了 clone 系统调用。初赛时由于要求测试样例要求比较低，在实现 clone 系统调用时并未完全利用用户传递的参数。我们根据 Linux manual page 中的要求，完善了内核的 fork 以及 TaskControlBlock 结构。\n// kernel/task/task.rs pub struct TaskControlBlock { ... pub sigactions: Arc\u0026lt;RwLock\u0026lt;[SigAction; MAX_SIGNUM as usize]\u0026gt;\u0026gt;, pub memory_set: Arc\u0026lt;RwLock\u0026lt;MemorySet\u0026gt;\u0026gt;, pub fd_table: Arc\u0026lt;RwLock\u0026lt;FDTable\u0026gt;\u0026gt;, pub robust_list: Arc\u0026lt;RwLock\u0026lt;RobustList\u0026gt;\u0026gt;, pub rlimit_nofile: Arc\u0026lt;RwLock\u0026lt;RLimit\u0026gt;\u0026gt;, inner: RwLock\u0026lt;TaskControlBlockInner\u0026gt;, } pub struct TaskControlBlockInner { ... pub pending_signals: SigSet, pub sigmask: SigMask, pub interval_timer: Option\u0026lt;IntervalTimer\u0026gt;, pub utime: TimeVal, pub stime: TimeVal, pub last_enter_umode_time: TimeVal, pub last_enter_smode_time: TimeVal, pub clear_child_tid: usize, /* CLONE_CHILD_CLEARTID */ } 相较于初赛，我们为 TCB 加入了有关信号、时间、资源等结构。并根据 sys_clone 传递的参数，正确地实现 fork，比如以下代码段：\n// kernel/src/task/task.rs(fn fork) // 拷贝用户地址空间 let memory_set = if flags.contains(CloneFlags::VM) { self.memory_set.clone() } else { Arc::new(RwLock::new(MemorySet::from_copy_on_write( \u0026amp;mut self.memory_set.write(), ))) }; if flags.contains(CloneFlags::THREAD) { memory_set.write().map_thread_trap_context(private_tid); } // kernel/syscall/impls/process.rs (sys_do_fork) if flags.contains(CloneFlags::PARENT_SETTID) { *translated_mut(current_user_token(), ptid as *mut u32) = new_pid as u32; } if flags.contains(CloneFlags::CHILD_SETTID) { *translated_mut(child_token, ctid as *mut u32) = new_pid as u32; } if flags.contains(CloneFlags::CHILD_CLEARTID) { new_task.inner_mut().clear_child_tid = ctid; } if flags.contains(CloneFlags::SETTLS) { let trap_cx = new_task.inner_mut().trap_context(); trap_cx.set_tp(tls); } 线程的引入 在 fork 过程中，当 CloneFlags 中存在 CLONE_THREAD 位时，正在创建的进程当前进程的为子线程\nPid 分配器的更改\n由于我们的内核在实现线程时，为了更方便地为子线程分配 TrapContext Frame 资源，我们规定子线程的 tid (pid) 不应该与小于进程（主线程）的 tid (pid) ，故移除了进程 pid 的回收操作。\n子进程与子线程的区分\n目前我们将子线程与子进程均保存在 TCB 的 children 字段，在遇到进程退出等问题时会判断 child 是子进程还是子线程。\n目前通过 tgid 与 pid 来区分 TCB 是父进程的子进程还是子线程\n// kernel/src/task/task.rs(fn fork) let pid_handle = pid_alloc(); let tgid = if flags.contains(CloneFlags::THREAD) { self.pid.0 } else { pid_handle.0 }; 如果 tgid 与 pid 值相同，则该 TCB 为进程，否则为线程\n为线程分配资源\n线程除了共享主线程（进程）的 memory_set, fd_table, sigaction 等资源，还需要一些独立的资源如ID, 内核地址空间的KernelStack，以及主线程中独立分配的 TrapContext Frame:\n// kernel/syscall/impls/process.rs (sys_do_fork) if flags.contains(CloneFlags::THREAD) { memory_set.write().map_thread_trap_context(private_tid); } // kernel/src/task/ttask.rs pub fn trap_context_position(tid: usize) -\u0026gt; VirtAddr { VirtAddr::from(TRAP_CONTEXT - tid * PAGE_SIZE) } 其中，private_tid 为tgid(主线程/父进程)与pid(子线程tid)的差值\n进程/线程的退出 当前进程结束的方式包括：\n进程运行完代码段，非法访问到 .rodata 引发 trap，在 trap 处理中回收进程对象 进程调用 exit 系统调用 进程收到 kill 相关的信号，在信号处理时退出 关于进程/线程的退出时需要做的工作包括：\n完成进程的初步回收： 将自身从 PID2TCB 映射管理器中移除 标记自身状态为 Zombie，记录退出码 将子进程移交给 initproc 如果自身为某进程的子线程，还需要找到主进程并将自身从主进程中移除，并压入子线程回收管理器 CHILDREN_THREAD_MONITOR 中，在下一次进程调度时回收可回收的资源 进程的父进程等待子进程退出，调用 wait 系统调用，完成子进程资源的回收：找到子进程中处于 Zombie 态的进程并且强引用计数为 1 的进程，移除该进程以彻底回收该进程的所有资源 子线程资源回收\n子线程退出时，子线程会加入到回收管理器 CHILDREN_THREAD_MONITOR 中，并在下一次进程调度时回收可回收的资源。\n// kernel/src/task/mod.rs (fn exit_current_and_run_next) if is_child_thread { let parent = inner.parent.as_ref().unwrap().upgrade().unwrap(); let mut parent_inner = parent.inner_mut(); let children_iter = parent_inner.children.iter(); let (idx, _) = children_iter .enumerate() .find(|(_, t)| t.pid() == pid) .unwrap(); parent_inner.children.remove(idx); drop(parent_inner); drop(parent); drop(inner); assert!(Arc::strong_count(\u0026amp;task) == 1); take_cancelled_chiled_thread(task); schedule(\u0026amp;mut TaskContext::empty() as *mut _); unreachable!() } 这个过程本身其实可以不用做，而是等主线程进行 wait 系统调用时彻底回收。但由于测试过程中，进程会创建成千上万个子线程，如果这些线程资源没有及时回收，如 TrapContext, KenerlStack 等资源，会浪费许多内存资源。\n其实 take_cancelled_chiled_thread(task)这段代码本身，以及 CHILDREN_THREAD_MONITOR变量，也就是说这段代码本身其实可以直接改为 drop(task)，因为此时 task 强引用计数一定为 1，task 中可以释放的资源都可以在, schedule 之前释放掉但是 task 在执行 exit_current_and_run_next时本身出于内核态，此时回收 task 的 KerenlStack 既不符合逻辑，又有可能产生一些隐患，故选择使用 CHILDREN_THREAD_MONITOR 在调度时释放退出的线程。\n","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/process-thread/","tags":["process","thread","全国赛I"],"title":"[全国赛I] 进程管理-进程与线程"},{"content":"一开始\n","date":"2023-07-12","permalink":"https://bitethedisk.github.io/post/opensbi-getchar/","tags":["全国赛I"],"title":"适配 OpenSBI"},{"content":"Rust 和 C 交互的时候需要注意 Rust 的堆，当从 Rust 程序 fork 出一个进程 来执行 C 程序的时候，C 是不能识别 Rust 的堆的(包括其堆管理器这些)，所以当 使用 C 程序去访问 fork 后的 Rust 堆上的对象，或者传递相关堆上对象的时候可能 会产生致命问题(如非法内存访问\u0026mdash;访问越界)\n","date":"2023-07-07","permalink":"https://bitethedisk.github.io/post/rust-heap-and-c/","tags":null,"title":"Rust Heap and C"},{"content":"随着开发的进行，我们需要的适配和封装的数据结构越来越多，其中大部分与我们的内核本体关系并没有那么紧密， 所以我们将这部分结构，如用于的引导程序、FAT32、Linux 相关数据结构放在了项目根目录中的 crates 里\nRust 本身是支持多个 crates 构成的一个 workspace，这些 crates 直接可以相互引用，但是由于我们使用 了 .cargo/config.toml 来配置 rustc，所以 workspace 并不能为我们所有 (因为目前 workspace 不支持在 workspace 中读取 .cargo/config.toml)\n使用 Git Submodule 管理测例 与区域赛不同，全国赛的测例数目较多，如果一旦发生更新构建起来也相对麻烦\n基于 Git Submodule 我们可以方便隔离当前 Git 仓库，做到依赖的隔离与同步\n就当前的实际环境来说:\ngit submodule add https://github.com/oscomp/testsuits-for-oskernel.git testsuits 上面的作用是将 testsuits-for-oskernel.git clone 到本地的 testsuits 文件夹中，后者会自动创建\n当重新拉取项目仓库时:\ngit submodule init git submodule update 就可以重新拉取 testsuits 中，仓库的内容了\n项目目录树 . ├── Makefile ├── README.md ├── crates │ ├── fat32/ ---- FAT32 读写库 │ ├── libd/ ---- libc 的~~后继者(划掉)~~ initproc，内核自动加载的第一个用户程序 │ ├── nix/ ---- Linux 相关数据结构 │ └── sync_cell/ ---- 实现了 Sync 的，具有内部可变性的 RefCell ├── docs/ ├── kernel/ │ ├── Makefile │ ├── build.rs ---- 用于监控相关文件，如 `crates/libd/bin/initproc.rs`，发生变化时重新编译 │ ├── cargo │ │ └── config.toml │ ├── linkerld │ │ └── linker.ld │ ├── src │ │ ├── boards │ │ │ └── qemu.rs ---- 平台相关参数 │ │ ├── console.rs │ │ ├── consts.rs │ │ ├── drivers │ │ ├── entry.S │ │ ├── error.rs │ │ ├── fs/ │ │ ├── logging.rs │ │ ├── macros │ │ │ ├── hsm.rs │ │ │ ├── mod.rs │ │ │ ├── on_boot.rs │ │ │ └── profile.rs ---- 用于打印某段代码运行时间的宏 │ │ ├── main.rs │ │ ├── mm │ │ ├── sbi.rs │ │ ├── syscall/ │ │ ├── task/ │ │ ├── timer.rs │ │ └── trap/ │ ├── target/ ---- 构建产物 │ └── vendor/ ---- 所有第三方依赖的本地归档 ├── testsuits/ ---- 通过 Git Submodule 内联的官方测例 └── workspace ---- 用于中间过程构建内核运行所需测例 1187 directories, 9162 files ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/project-structure/","tags":["全国赛I","RustWorkspace"],"title":"[全国赛I]项目结构(rust-workspace不能使用)"},{"content":"项目提供了一系列的 Makefile 来简化开发流程.\n一般只会用到项目根目录中的 Makefile:\nBOOTLOADER_ELF = ./kernel/bootloader/rustsbi-qemu KERNEL_ELF = ./kernel/target/riscv64gc-unknown-none-elf/release/kernel sbi-qemu: @cp $(BOOTLOADER_ELF) sbi-qemu kernel-qemu: @mv kernel/cargo kernel/.cargo @cd kernel/ \u0026amp;\u0026amp; make kernel @cp $(KERNEL_ELF) kernel-qemu all: sbi-qemu kernel-qemu clean: @rm -f kernel-qemu @rm -f sbi-qemu @rm -rf build/ @rm -rf temp/ @cd kernel/ \u0026amp;\u0026amp; cargo clean @cd workspace/ \u0026amp;\u0026amp; make clean @cd fat32/ \u0026amp;\u0026amp; cargo clean @cd misc/ \u0026amp;\u0026amp; make clean fat32img: @cd kernel/ \u0026amp;\u0026amp; make fat32img run: @cd kernel/ \u0026amp;\u0026amp; make run debug-server: @cd kernel/ \u0026amp;\u0026amp; make debug-server debug: @cd kernel/ \u0026amp;\u0026amp; make debug 这里只需关注以下几点:\nrun: 构建内核并运行，内核是以 release 方式构建的 debug-server: 以 debug 方式构建内核并运行 gdb debug server debug: 链接上面运行的 debug server 开始调试 ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/about-makefile/","tags":["Makefile","Debug","区域赛"],"title":"[区域赛] 有关 BTD 的 Makefile"},{"content":"在 BTD 的开发中经常会遇到 debug 的需求，BTD 提供了一个方便的调试流程，只需启动两个 shell 即可 开始调试\n下面将演示一次具体的调试流程\n启动两个 shell，在项目根目录下分别运行 make debug-server 和 make debug， 之后只需要在运行 make debug 的 shell 中执行接下来的命令即可 # 从当前 pc 开始显示 11 条指令，11 条是不发生折叠的极限（也可能只是我这是这样） (gdb) x/11i $pc (gdb) b *0x80000000 # 在内核第一条指令处打个断点 (gdb) c # continue 执行到断点处 (gdb) si # si 单步执行 (step in，会嵌入函数具体流程中) 需要注意的是，在多核运行时，代码执行过程中会出现系统线程的切换，例如下面这样：\n(gdb) c Continuing. [Switching to Thread 1.2] 这时候我们要看当前线程的状态，根据当前状态进行调试，该打断点的地方不要忘了打断点，不然会跑飞\n像是这样，只能用 Ctrl + C 来掐死 (主要是我也没去找其他可能的方法 😛):\n看到地址了吗，直接归零，而且这块地址是不可访问的，只能卡在这了，除非扬了当前的 shell 重来（大概\nThread 2 received signal SIGINT, Interrupt. 0x0000000000000000 in ?? () (gdb) x/11i $pc =\u0026gt; 0x0: Cannot access memory at address 0x0 (gdb) ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/how-to-debug-the-kernel/","tags":["OSKernel","Debug"],"title":"开发中如调试内核"},{"content":"内核在运行的时候总是会不知何时卡死，底层原因是持续触发时钟中断\n// 时间片到了 Trap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { set_next_trigger(); // 主要是这里 suspend_current_and_run_next(); } 初步解决 我们将设置下一个时钟中断放置在了 suspend_current_and_run_next 之前，导致可能因为后者执行 时间过长而使用户态一直处于时钟中断触发状态，至于为什么会在 RV64 上一直触发中断，可以参阅 RV 的特权级手册:\nPlatforms provide a real-time counter, exposed as a memory-mapped machine-mode read-write register, mtime. mtime must increment at constant frequency, and the platform must provide a mechanism for determining the period of an mtime tick. The mtime register will wrap around if the count overflows.\nThe mtime register has a 64-bit precision on all RV32 and RV64 systems. Platforms provide a 64- bit memory-mapped machine-mode timer compare register (mtimecmp). A machine timer interrupt becomes pending whenever mtime contains a value greater than or equal to mtimecmp, treating the values as unsigned integers. The interrupt remains posted until mtimecmp becomes greater than mtime (typically as a result of writing mtimecmp). The interrupt will only be taken if interrupts are enabled and the MTIE bit is set in the mie register.\n由于 suspend_current_and_run_next 执行的时间超过了一个时间片的长度，导致其返回用户态进程时， mtime 的值已经大于了 set_next_trigger 设置的时间点，由上文可得，如果 mtime 大于等于 mtimecmp(即 set_next_trigger 设置的值)，并且 mie 为使能状态，那么时钟中断会一直处于触发状态.\n而我们的内核 mie 一直处于使能状态，所以 S 态的时钟中断会持续在用户态发生(S 态中断不会打断同级与 更高特权级代码的执行)，导致用户态毫无进展，而我们内核的引导程序 initproc 会一直等待卡死用户进程 变为僵尸态，所以造成了内核执行流的卡死.\n解决办法:\n简单调整下位置\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); set_next_trigger(); } 但这样真的对吗? 不对，因为会导致用户态程序卡死整个内核的执行流\n一个致命的缺点是，用户态的程序需要第一次运行后才能正确的获取时钟中断，不然只能等轮回一边后才可能正确让出\n当前的逻辑是:\nRustSBI 完成初始化后，在 meow(没错，这是我们 Rust 代码的 ENTRYPOINT)，中初步设定一个时钟中断\n#[cfg(not(feature = \u0026quot;multi_harts\u0026quot;))] #[no_mangle] pub fn meow() -\u0026gt; ! { if hartid!() == 0 { init_bss(); unsafe { set_fs(FS::Dirty) } lang_items::setup(); logging::init(); mm::init(); trap::init(); trap::enable_stimer_interrupt(); trap::set_next_trigger(); fs::init(); task::add_initproc(); task::run_tasks(); } else { loop {} } unreachable!(\u0026quot;main.rs/meow: you should not be here!\u0026quot;); } 这是第一个问题，我们原本想的是，这个时钟中断会在第一用户态程序运行时发生，但是有可能它在 fs::init() 或者 task::add_initproc() 中已经发生了，这会导致一进入用户态程序就发生中断，这和我们 预期的不一样.\n而且，陷入中断后，除非使失能 mie，或者再次 set_next_trigger()(又或者 mtime 发生回环)， 否则将一直处于中断触发的状态\n而这之后切换的用户进程都会遇到中断而直接返回，直到运行到第一个用户进程(其实应该是引导程序 initproc)， 在下面 suspend_current_and_run_next 真正意义上的返回后，重新设置下一个中断时间点，这才能让 OS 内核 所有的用户进程进入正常的运行流.\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); set_next_trigger(); } 而这种时间上的开销显然是没必要的，所以我们根据所有用户进程都会通过 trap_return 返回用户态这一点， 将 set_next_trigger 设置在了 trap_return 中，同时判断当前进程是否是因为时间片耗尽而导致的 trap 返回:\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); } // ... #[no_mangle] pub fn trap_return() -\u0026gt; ! { // ... if is_time_intr_trap() { set_next_trigger(); } // ... } /// 是否是由于时间片耗尽导致的 trap fn is_time_intr_trap() -\u0026gt; bool { let scause = scause::read(); scause.cause() == Trap::Interrupt(scause::Interrupt::SupervisorTimer) } 最终结果 我们消除了一个严重的 bug: 内核在执行用户程序时随机卡死 删去了 meow 中不利于系统鲁棒性的代码 ","date":"2023-06-29","permalink":"https://bitethedisk.github.io/post/random-stuck/","tags":["Problems-\u0026-Solutions"],"title":"[已解决]内核在执行用户程序时随机卡死"}]
=======
[{"content":"文件系统优化历程 我们在 FAT32 设计上，采用了 rCore-Tutorial easy-fs 相同的松耦合模块化设计思路，与底层设备驱动之间通过抽象接口 BlockDevice 来连接，避免了与设备驱动的绑定。FAT32 库通过 Rust 提供的 alloc crate 来隔离了操作系统内核的内存管理，避免了直接调用内存管理的内核函数。 同时在设计中避免了直接访问进程相关的数据和函数，从而隔离了操作系统内核的进程管理。\n虽然我们在内核中给出了虚拟文件的抽象 File Trait，但是由于只完成的 FAT32 文件系统，内核中不存在虚拟文件系统这一抽象，也没有 Inode 层面的缓存，内核中的文件实际上将 FAT32 提供的 VirtFile 进一步封装成 KFile，这导致每次写入数据都会同步到磁盘，此时 BlockCache 缓存块的作用微乎其微。\n第一阶段 —— Static BusyBox 在全国赛第一阶段时，我们在测试过程中发现内核跑测例的执行速度非常缓慢， 比如启动 busybox 都需要花半分钟、iozone 测试中，读写速度大概为几十到几百 KB/s，显然内核的文件读写性能非常糟糕，导致没法跑完我们已经实现的测试。\n此时为了尽可能跑完测试，我们发现由于大部分测试需要使用 busybox，为了避免多次解析 elf、 从零创建地址空间等问题，我们采用了类似于加载 initproc 的方法。 具体而言，我们将 busybox 预加载到内核中，并保存 load_elf 获取的信息。每次执行 busybox 时， 我们直接使用保存的 load_elf 信息，并通过写时拷贝来创建所需的 busybox 进程的地址空间，更快速地创建 busybox。\n// 第一阶段 submit 分支 pub static ref BUSYBOX: RwLock\u0026lt;Busybox\u0026gt; = RwLock::new({ extern \u0026quot;C\u0026quot; { fn busybox_entry(); fn busybox_tail(); } let entry = busybox_entry as usize; let tail = busybox_tail as usize; let siz = tail - entry; let busybox = unsafe { core::slice::from_raw_parts(entry as *const u8, siz) }; let path = AbsolutePath::from_str(\u0026quot;/busybox0\u0026quot;); let inode = fs::open(path, OpenFlags::O_CREATE, CreateMode::empty()).expect(\u0026quot;busybox0 create failed\u0026quot;); inode.write_all(\u0026amp;busybox.to_owned()); let bb = Arc::new(TaskControlBlock::new(inode.clone())); inode.delete(); Busybox { inner: bb, } }); pub static mut ONCE_BB_ENTRY: usize = 0; pub static mut ONCE_BB_AUX: Vec\u0026lt;AuxEntry\u0026gt; = Vec::new(); pub struct Busybox { inner: Arc\u0026lt;TaskControlBlock\u0026gt;, } impl Busybox { pub fn elf_entry_point(\u0026amp;self) -\u0026gt; usize { unsafe { ONCE_BB_ENTRY } } pub fn aux(\u0026amp;self) -\u0026gt; Vec\u0026lt;AuxEntry\u0026gt; { unsafe { ONCE_BB_AUX.clone() } } pub fn memory_set(\u0026amp;self) -\u0026gt; MemorySet { let mut write = self.inner.memory_set.write(); MemorySet::from_copy_on_write(\u0026amp;mut write) } } 虽然对于当时的我们来说算是雪中送炭。但这实际上并非合理的设计。\n第一阶段后 —— 分析 FAT32，改造簇链 在第一阶段结束后，我们通过追踪读写相关代码所耗费的时间, 比如：\n// 当前 pub fn write_at(\u0026amp;self, offset: usize, buf: \u0026amp;[u8]) -\u0026gt; usize { #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] time_trace!(\u0026quot;write_at\u0026quot;); ... } pub fn read_at(\u0026amp;self, offset: usize, buf: \u0026amp;mut [u8]) -\u0026gt; usize { #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] time_trace!(\u0026quot;read_at\u0026quot;); } #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] start_trace!(\u0026quot;cluster\u0026quot;); let pre_cluster_cnt = offset / cluster_size; #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] start_trace!(\u0026quot;clone\u0026quot;); let clus_chain = self.cluster_chain.read(); #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] end_trace!(); let mut cluster_iter = clus_chain.cluster_vec.iter().skip(pre_cluster_cnt); #[cfg(feature = \u0026quot;time-tracer\u0026quot;)] end_trace!(); (TimeTracer 用于记录执行到当前代码的时的系统时间, 当 Timetracer drop 时统计期间消耗的时间)\n首先定位到影响 FAT32 文件直接读写文件过程中最为耗时的操作：遍历文件的簇链过程中，不断在 FAT 表中查询下一簇的位置。我们实现的 FAT32 文件系统所有读写，包括遍历簇链的操作都是依赖 BlockCache 提供的 get_block_cache 方法，但其中遍历簇链这个操作非常的频繁，导致整个读写过程耗时严重。\n读写文件时：\n// 第一阶段 submit 分支 while index \u0026lt; end { let cluster_offset_in_disk = self.fs.read().bpb.offset(curr_cluster); let start_block_id = cluster_offset_in_disk / BLOCK_SIZE; for block_id in start_block_id..start_block_id + spc { ... } if index \u0026gt;= end { break; } curr_cluster = self.fs.read() .fat.read() .get_next_cluster(curr_cluster).unwrap(); curr_cluster = clus_chain.current_cluster; } // 第一阶段 submit 分支 pub fn get_next_cluster(\u0026amp;self, cluster: u32) -\u0026gt; Option\u0026lt;u32\u0026gt; { let (block_id, offset_in_block) = self.cluster_id_pos(cluster); let next_cluster: u32 = get_block_cache(block_id, Arc::clone(\u0026amp;self.device)) .read() .read(offset_in_block, |\u0026amp;next_cluster: \u0026amp;u32| next_cluster); assert!(next_cluster \u0026gt;= 2); if next_cluster \u0026gt;= END_OF_CLUSTER { None } else { Some(next_cluster) } } 基本流程为：IMAGE\n由于该操作对于同一文件的读写非常频繁，并且大部分情况下，特别是多次读文件时，所获取的簇链信息不变，但每次都要重新从 BlockCache 中读取，显然这个过程是可以优化的。一个非常容易想到的方法就是：\n在打开文件时，预读簇链，将簇链信息保存下来 对于可能会修改簇链的文件写操作，如在文件 increase_size 时，重新读取簇链 于是我们对 FAT32 文件系统的簇链设计做出修改：\n// 当前 pub struct ClusterChain { ... pub(crate) cluster_vec: Vec\u0026lt;u32\u0026gt;, } // 当前 // open / increase_size 时 loop { ... let next_cluster = get_block_cache(block_id, Arc::clone(\u0026amp;self.device)) .read() .read(offset_left, |\u0026amp;cluster: \u0026amp;u32| cluster); if next_cluster \u0026gt;= END_OF_CLUSTER { break; } else { self.cluster_vec.push(next_cluster); }; } 对比视频：IMAGE\n第一阶段后 —— 更进一步，Page Cache 其实经过以上优化，内核运行测试时的速度已经能够接受了。但是问题的关键还是在于上面提到的：由于只完成的 FAT32 文件系统，内核中不存在虚拟文件系统这一抽象，也没有 Inode 层面的缓存，内核中的文件实际上将 FAT32 提供的 VirtFile 进一步封装成 KFile，这导致每次写入数据都会同步到磁盘。\n文件读写时，大量的磁盘直接 IO，才是急需解决的问题。由于测试过程中创建的文件的读写操作实际上是在内存中通过 Page Cache 进行的，往往不会直接写回文件系统，特别是在单核下，大量的对磁盘的直接读写会导致内核执行速度变慢（实际我们在多核下测试开启写回操作对速度影响不大）。\n我们首先想到了可以通过实现 TempFS (将文件存储在系统的内存中而不是磁盘上)，将测试过程中内核关于文件的操作如创建，读写等，都交给 TempFS，不必每次都写回 FAT32。或者是在目前的 KFile 基础上加上 PageCache 减少对磁盘的频繁访问。\n由于缺乏相关实现经验，查看了部分第一阶段提交时的优秀队伍的是否有做相关工作，希望从中学到实现思路。\n最后我们选择参考同一期的优秀队伍 TitanixOS 的 PageCache 实现思路，为内核加入 PageCache 机制。\n相关结构如下：\npub struct KFile { // read only feilds readable: bool, writable: bool, path: AbsolutePath, name: String, // shared by some files (uaually happens when fork) pub time_info: Mutex\u0026lt;InodeTime\u0026gt;, pub offset: Mutex\u0026lt;usize\u0026gt;, pub flags: Mutex\u0026lt;OpenFlags\u0026gt;, pub available: Mutex\u0026lt;bool\u0026gt;, // shared by the same file (with page cache) pub inode: Arc\u0026lt;Inode\u0026gt;, } pub struct Inode { pub file: Mutex\u0026lt;Arc\u0026lt;VirtFile\u0026gt;\u0026gt;, fid: u64, #[cfg(not(feature = \u0026quot;no-page-cache\u0026quot;))] pub page_cache: Mutex\u0026lt;Option\u0026lt;Arc\u0026lt;PageCache\u0026gt;\u0026gt;\u0026gt;, #[cfg(not(feature = \u0026quot;no-page-cache\u0026quot;))] pub file_size: Mutex\u0026lt;usize\u0026gt;, } 关于 Inode 的设计：\n配合 InodeCache 加快查找效率，实现文件的缓存 file 字段为 FAT32 提供的 VirtFile 关于在 Inode 中保存文件的大小的理由: 测试过程中创建的文件的读写操作实际上是在内存中通过 Page Cache 进行的，往往不会直接写回文件系统。为了提高测试速度，我们使用 Rust Feature 机制，默认关闭 Inode Drop 时的写回操作，相当于借助 FAT32 VirtFile 的 ”外壳“ 实现了一个不算标准的 TempFS； 文件读写过程中需要用到 file_size 参数，加上不会每次写文件或关闭文件后直接写回文件系统， 故不能通过在文件系统中读取文件大小的方式来获取文件大小（不一致）； 不同进程对该文件进行写操作时会改写文件的大小，使用 Inode Cache 再次打开文件时必须保证文件大小的一致性。 pub struct PageCache { inode: Option\u0026lt;Weak\u0026lt;VirtFile\u0026gt;\u0026gt;, // page number -\u0026gt; page pub pages: RwLock\u0026lt;BTreeMap\u0026lt;usize, Arc\u0026lt;FilePage\u0026gt;\u0026gt;\u0026gt;, } pub struct FilePage { pub permission: MapPermission, pub data_frame: FrameTracker, pub file_info: Option\u0026lt;Mutex\u0026lt;FilePageInfo\u0026gt;\u0026gt;, } pub struct FilePageInfo { /// 页的起始位置在文件中的偏移（一定是页对齐的） pub file_offset: usize, pub data_states: [DataState; PAGE_SIZE / BLOCK_SIZE], inode: Weak\u0026lt;VirtFile\u0026gt;, } 文件操作图片：IMG\n对比图片：IMG\n至此，我们的内核文件优化过程暂时告一段落。\n","date":"2023-08-17","permalink":"https://bitethedisk.github.io/post/fs-optimization/","tags":["优化"],"title":"文件系统优化历程"},{"content":"在目前的实现中信号总共有63种，1-31为非实时信号，34-63是实时信号。32和33为未定义信号。 信号是每个进程独有的，除此之外每个进程还有信号掩码。\n涉及信号处理的共有3个系统调用：SYS_SIGACTION，SYS_SIGPROCMASK，SYS_SIGRETURN。\n涉及的系统调用 sys_sigaction用于为一个信号注册信号处理函数，当进程接受到信号后会跳转到这个信号处理函数。 一般信号处理函数后会调用sigreturn将程序上下文恢复到执行信号处理函数之前的状态。 但程序也可能不调用sigreturn，而是使用longjump跳转到别的位置，内核不关心信号处理函数是否返回， 不在内核中维护信号处理上下文信息，而是在执行信号处理函数之前将上下文压入用户栈，并在sigreturn时从用户栈中恢复信息。 在执行信号处理函数的过程中，这些保存的上下文信息可能被用户程序修改，用户程序可以借此返回到不同的地方。\nsys_sigprocmask用于修改进程的信号掩码。信号掩码可以用来屏蔽信号，被屏蔽的信号会被阻塞， 直到信号掩码不再阻塞该信号时该信号才会被处理。 9号信号和19号信号不能被阻塞。 sys_sigreturn会从用户栈中取出上下文信息将程序恢复到信号处理前的状态。 sys_sigreturn没有返回值，在执行完后不应将执行结果写入a0寄存器。\n由于执行信号处理函数时与执行其他用户函数没有区别，linux的信号设计自然支持信号处理的嵌套， 只要正确实现了这几个系统调用，无需在内核内保存额外信息就可以支持信号的嵌套调用。 一般信号的产生是通过sys_kill等系统调用产生的，除此之外进程退出时也会产生信号。 信号处理的时机是不确定的。现在内核中会在每次返回用户态之前检查有无需要处理的信号。\n与其他系统调用的交互 sys_wait4 、sys_read、sys_futex等具有阻塞等待行为的系统调用可以被信号中断， 如果在阻塞过程中有到来的信号应该停止等待，返回被中断的错误码。 fork 出来的子进程应该继承父进程的注册的信号处理程序，和信号掩码。 exec 后程序应该清空信号处理程序但是保留信号掩码。\n","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/signal/","tags":["全国赛第一阶段"],"title":"[全国赛I] 信号处理"},{"content":"由于大部分测试需要使用 busybox，为了避免多次解析 elf、从零创建地址空间等问题，我们采用了类似于加载initproc的方法。\n具体而言，我们将 busybox 预加载到内核中，并保存 load_elf 获取的信息。每次执行busybox时，我们直接使用保存的 load_elf 信息，并通过写时拷贝来创建所需的 busybox 进程的地址空间，更快速地创建 busybox 进程从而实现更高效的测试。\n// kernel/src/task/initproc/mod.rs pub static ref BUSYBOX: RwLock\u0026lt;Busybox\u0026gt; = RwLock::new({ extern \u0026quot;C\u0026quot; { fn busybox_entry(); fn busybox_tail(); } let entry = busybox_entry as usize; let tail = busybox_tail as usize; let siz = tail - entry; let busybox = unsafe { core::slice::from_raw_parts(entry as *const u8, siz) }; let path = AbsolutePath::from_str(\u0026quot;/busybox0\u0026quot;); let inode = fs::open(path, OpenFlags::O_CREATE, CreateMode::empty()).expect(\u0026quot;busybox0 create failed\u0026quot;); inode.write_all(\u0026amp;busybox.to_owned()); let bb = Arc::new(TaskControlBlock::new(inode.clone())); inode.delete(); Busybox { inner: bb, } }); pub static mut ONCE_BB_ENTRY: usize = 0; pub static mut ONCE_BB_AUX: Vec\u0026lt;AuxEntry\u0026gt; = Vec::new(); pub struct Busybox { inner: Arc\u0026lt;TaskControlBlock\u0026gt;, } impl Busybox { pub fn elf_entry_point(\u0026amp;self) -\u0026gt; usize { unsafe { ONCE_BB_ENTRY } } pub fn aux(\u0026amp;self) -\u0026gt; Vec\u0026lt;AuxEntry\u0026gt; { unsafe { ONCE_BB_AUX.clone() } } pub fn memory_set(\u0026amp;self) -\u0026gt; MemorySet { let mut write = self.inner.memory_set.write(); MemorySet::from_copy_on_write(\u0026amp;mut write) } } ","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/busybox-pre-init/","tags":["全国赛第一阶段"],"title":"[全国赛I] busybox 预加载"},{"content":"在我们的内核中，我们使用 TASKMANAGER 管理分别处于就绪态，阻塞态的进程，包括因为调用 nanosleep 而休眠的进程。\n// kernel/src/task/manager.rs // 负责管理待调度的进程对象 pub struct TaskManager { ready_queue: VecDeque\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, waiting_queue: VecDeque\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, hq: BinaryHeap\u0026lt;HangingTask\u0026gt;, } // 用于管理 sleep 进程 pub struct HangingTask { wake_up_time: usize, // ns inner: Arc\u0026lt;TaskControlBlock\u0026gt;, } // 用于处理子线程的资源回收 pub struct ChildrenThreadMonitor { cancelled_child_threads: Vec\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, } // 维护内核中 pid 到 TCB 的映射 pub static ref PID2TCB: Mutex\u0026lt;BTreeMap\u0026lt;usize, Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;\u0026gt; = Mutex::new(BTreeMap::new()); // 子线程回收管理器 pub static CHILDREN_THREAD_MONITOR: Mutex\u0026lt;ChildrenThreadMonitor\u0026gt; = Mutex::new(ChildrenThreadMonitor::new()); // kernel/src/task/processor/processor.rs /// [`Processor`] 是描述 CPU执行状态 的数据结构。 /// 在单核环境下，我们仅创建单个 Processor 的全局实例 PROCESSOR pub static mut PROCESSOR: SyncRefCell\u0026lt;Processor\u0026gt; = SyncRefCell::new(Processor::new()); /// 每个核上的处理器，负责运行一个进程 pub struct Processor { /// 当前处理器上正在执行的任务 current: Option\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, /// 当前处理器上的 idle 控制流的任务上下文 idle_task_cx: TaskContext, } run_tasks 分别尝试从 hang_task, ready_task 队列中获取进程调度。\n// kernel/src/task/processor/schedule.rs /// 进入 idle 控制流，它运行在这个 CPU 核的启动栈上， /// 功能是循环调用 fetch_task 直到顺利从任务管理器中取出一个任务，随后便准备通过任务切换的方式来执行 pub fn run_tasks() { let bb = BUSYBOX.read(); // lazy static busybox drop(bb); loop { let processor = acquire_processor(); recycle_child_threads_res(); if let Some(hanging_task) = check_hanging() { run_task(hanging_task, processor); } else if let Some(interupt_task) = check_futex_interupt_or_expire() { unblock_task(interupt_task); } else if let Some(task) = fetch_task() { run_task(task, processor); } } } 值得一提的是，我们在进程调度时还需要检测 block_task 队列中， 因为在系统调用过程中被信号打断的 task 是否有处理完信号，或者 futex_wait 时给出的 timeout 是否已超时以唤醒该进程并加入到 ready_task 中。\n// kernel/src/task/manager.rs pub fn check_futex_interupt_or_expire(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt; { for tcb in self.waiting_queue.iter() { let lock = tcb.inner_ref(); // 被信号打断的 task 是否有处理完信号 if !lock.pending_signals.difference(lock.sigmask).is_empty() { return Some(tcb.clone()); } } let mut global_futex_que = FUTEX_QUEUE.write(); for (_, futex_queue) in global_futex_que.iter_mut() { // timeout 是否已超时 if let Some(task) = futex_queue.pop_expire_waiter() { return Some(task.clone()); } } None } ","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/scheduling/","tags":["进程","线程","全国赛第一阶段"],"title":"[全国赛I] 进程/线程调度"},{"content":"TCB 与 fork 的更改 相较于初赛，完善了 clone 系统调用。初赛时由于要求测试样例要求比较低，在实现 clone 系统调用时并未完全利用用户传递的参数。我们根据 Linux manual page 中的要求，完善了内核的 fork 以及 TaskControlBlock 结构。\n// kernel/task/task.rs pub struct TaskControlBlock { ... pub sigactions: Arc\u0026lt;RwLock\u0026lt;[SigAction; MAX_SIGNUM as usize]\u0026gt;\u0026gt;, pub memory_set: Arc\u0026lt;RwLock\u0026lt;MemorySet\u0026gt;\u0026gt;, pub fd_table: Arc\u0026lt;RwLock\u0026lt;FDTable\u0026gt;\u0026gt;, pub robust_list: Arc\u0026lt;RwLock\u0026lt;RobustList\u0026gt;\u0026gt;, pub rlimit_nofile: Arc\u0026lt;RwLock\u0026lt;RLimit\u0026gt;\u0026gt;, inner: RwLock\u0026lt;TaskControlBlockInner\u0026gt;, } pub struct TaskControlBlockInner { ... pub pending_signals: SigSet, pub sigmask: SigMask, pub interval_timer: Option\u0026lt;IntervalTimer\u0026gt;, pub utime: TimeVal, pub stime: TimeVal, pub last_enter_umode_time: TimeVal, pub last_enter_smode_time: TimeVal, pub clear_child_tid: usize, /* CLONE_CHILD_CLEARTID */ } 相较于初赛，我们为 TCB 加入了有关信号、时间、资源等结构。并根据 sys_clone 传递的参数，正确地实现 fork，比如以下代码段：\n// kernel/src/task/task.rs(fn fork) // 拷贝用户地址空间 let memory_set = if flags.contains(CloneFlags::VM) { self.memory_set.clone() } else { Arc::new(RwLock::new(MemorySet::from_copy_on_write( \u0026amp;mut self.memory_set.write(), ))) }; if flags.contains(CloneFlags::THREAD) { memory_set.write().map_thread_trap_context(private_tid); } // kernel/syscall/impls/process.rs (sys_do_fork) if flags.contains(CloneFlags::PARENT_SETTID) { *translated_mut(current_user_token(), ptid as *mut u32) = new_pid as u32; } if flags.contains(CloneFlags::CHILD_SETTID) { *translated_mut(child_token, ctid as *mut u32) = new_pid as u32; } if flags.contains(CloneFlags::CHILD_CLEARTID) { new_task.inner_mut().clear_child_tid = ctid; } if flags.contains(CloneFlags::SETTLS) { let trap_cx = new_task.inner_mut().trap_context(); trap_cx.set_tp(tls); } 线程的引入 在 fork 过程中，当 CloneFlags 中存在 CLONE_THREAD 位时，正在创建的进程当前进程的为子线程\nPid 分配器的更改\n由于我们的内核在实现线程时，为了更方便地为子线程分配 TrapContext Frame 资源，我们规定子线程的 tid (pid) 不应该与小于进程（主线程）的 tid (pid) ，故移除了进程 pid 的回收操作。\n子进程与子线程的区分\n目前我们将子线程与子进程均保存在 TCB 的 children 字段，在遇到进程退出等问题时会判断 child 是子进程还是子线程。\n目前通过 tgid 与 pid 来区分 TCB 是父进程的子进程还是子线程\n// kernel/src/task/task.rs(fn fork) let pid_handle = pid_alloc(); let tgid = if flags.contains(CloneFlags::THREAD) { self.pid.0 } else { pid_handle.0 }; 如果 tgid 与 pid 值相同，则该 TCB 为进程，否则为线程\n为线程分配资源\n线程除了共享主线程（进程）的 memory_set, fd_table, sigaction 等资源，还需要一些独立的资源如ID, 内核地址空间的KernelStack，以及主线程中独立分配的 TrapContext Frame:\n// kernel/syscall/impls/process.rs (sys_do_fork) if flags.contains(CloneFlags::THREAD) { memory_set.write().map_thread_trap_context(private_tid); } // kernel/src/task/ttask.rs pub fn trap_context_position(tid: usize) -\u0026gt; VirtAddr { VirtAddr::from(TRAP_CONTEXT - tid * PAGE_SIZE) } 其中，private_tid 为tgid(主线程/父进程)与pid(子线程tid)的差值\n进程/线程的退出 当前进程结束的方式包括：\n进程运行完代码段，非法访问到 .rodata 引发 trap，在 trap 处理中回收进程对象 进程调用 exit 系统调用 进程收到 kill 相关的信号，在信号处理时退出 关于进程/线程的退出时需要做的工作包括：\n完成进程的初步回收： 将自身从 PID2TCB 映射管理器中移除 标记自身状态为 Zombie，记录退出码 将子进程移交给 initproc 如果自身为某进程的子线程，还需要找到主进程并将自身从主进程中移除，并压入子线程回收管理器 CHILDREN_THREAD_MONITOR 中，在下一次进程调度时回收可回收的资源 进程的父进程等待子进程退出，调用 wait 系统调用，完成子进程资源的回收：找到子进程中处于 Zombie 态的进程并且强引用计数为 1 的进程，移除该进程以彻底回收该进程的所有资源 子线程资源回收\n子线程退出时，子线程会加入到回收管理器 CHILDREN_THREAD_MONITOR 中，并在下一次进程调度时回收可回收的资源。\n// kernel/src/task/mod.rs (fn exit_current_and_run_next) if is_child_thread { let parent = inner.parent.as_ref().unwrap().upgrade().unwrap(); let mut parent_inner = parent.inner_mut(); let children_iter = parent_inner.children.iter(); let (idx, _) = children_iter .enumerate() .find(|(_, t)| t.pid() == pid) .unwrap(); parent_inner.children.remove(idx); drop(parent_inner); drop(parent); drop(inner); assert!(Arc::strong_count(\u0026amp;task) == 1); take_cancelled_chiled_thread(task); schedule(\u0026amp;mut TaskContext::empty() as *mut _); unreachable!() } 这个过程本身其实可以不用做，而是等主线程进行 wait 系统调用时彻底回收。 但由于测试过程中，进程会创建成千上万个子线程，如果这些线程资源没有及时回收，如 TrapContext, KenerlStack 等资源，会浪费许多内存资源。\n其实 take_cancelled_chiled_thread(task)这段代码本身，以及 CHILDREN_THREAD_MONITOR变量，也就是说这段代码本身其实可以直接改为 drop(task)， 因为此时 task 强引用计数一定为 1，task 中可以释放的资源都可以在, schedule 之前释放掉但是 task 在执行 exit_current_and_run_next时本身出于内核态，此时回收 task 的 KerenlStack 既不符合逻辑，又有可能产生一些隐患，故选择使用 CHILDREN_THREAD_MONITOR 在调度时释放退出的线程。\n","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/process-thread/","tags":["进程","线程","全国赛第一阶段"],"title":"[全国赛I] 进程管理-进程与线程"},{"content":"随着开发的进行，我们需要的适配和封装的数据结构越来越多，其中大部分与我们的内核本体关系并没有那么紧密， 所以我们将这部分结构，如用于的引导程序、FAT32、Linux 相关数据结构放在了项目根目录中的 crates 里\nRust 本身是支持多个 crates 构成的一个 workspace，这些 crates 直接可以相互引用，但是由于我们使用 了 .cargo/config.toml 来配置 rustc，所以 workspace 并不能为我们所有 (因为目前 workspace 不支持在 workspace 中读取 .cargo/config.toml)\n使用 Git Submodule 管理测例 与区域赛不同，全国赛的测例数目较多，如果一旦发生更新构建起来也相对麻烦\n基于 Git Submodule 我们可以方便隔离当前 Git 仓库，做到依赖的隔离与同步\n就当前的实际环境来说:\ngit submodule add https://github.com/oscomp/testsuits-for-oskernel.git testsuits 上面的作用是将 testsuits-for-oskernel.git clone 到本地的 testsuits 文件夹中，后者会自动创建\n当重新拉取项目仓库时:\ngit submodule init git submodule update 就可以重新拉取 testsuits 中，仓库的内容了\n项目目录树 . ├── Makefile ├── README.md ├── crates │ ├── fat32/ ---- FAT32 读写库 │ ├── libd/ ---- libc 的~~后继者(划掉)~~ initproc，内核自动加载的第一个用户程序 │ ├── nix/ ---- Linux 相关数据结构 │ └── sync_cell/ ---- 实现了 Sync 的，具有内部可变性的 RefCell ├── docs/ ├── kernel/ │ ├── Makefile │ ├── build.rs ---- 用于监控相关文件，如 `crates/libd/bin/initproc.rs`，发生变化时重新编译 │ ├── cargo │ │ └── config.toml │ ├── linkerld │ │ └── linker.ld │ ├── src │ │ ├── boards │ │ │ └── qemu.rs ---- 平台相关参数 │ │ ├── console.rs │ │ ├── consts.rs │ │ ├── drivers │ │ ├── entry.S │ │ ├── error.rs │ │ ├── fs/ │ │ ├── logging.rs │ │ ├── macros │ │ │ ├── hsm.rs │ │ │ ├── mod.rs │ │ │ ├── on_boot.rs │ │ │ └── profile.rs ---- 用于打印某段代码运行时间的宏 │ │ ├── main.rs │ │ ├── mm │ │ ├── sbi.rs │ │ ├── syscall/ │ │ ├── task/ │ │ ├── timer.rs │ │ └── trap/ │ ├── target/ ---- 构建产物 │ └── vendor/ ---- 所有第三方依赖的本地归档 ├── testsuits/ ---- 通过 Git Submodule 内联的官方测例 └── workspace ---- 用于中间过程构建内核运行所需测例 1187 directories, 9162 files ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/project-structure/","tags":["全国赛第一阶段"],"title":"[全国赛I] 项目结构(rust-workspace不能使用)"},{"content":"项目提供了一系列的 Makefile 来简化开发流程.\n一般只会用到项目根目录中的 Makefile:\nBOOTLOADER_ELF = ./kernel/bootloader/rustsbi-qemu KERNEL_ELF = ./kernel/target/riscv64gc-unknown-none-elf/release/kernel sbi-qemu: @cp $(BOOTLOADER_ELF) sbi-qemu kernel-qemu: @mv kernel/cargo kernel/.cargo @cd kernel/ \u0026amp;\u0026amp; make kernel @cp $(KERNEL_ELF) kernel-qemu all: sbi-qemu kernel-qemu clean: @rm -f kernel-qemu @rm -f sbi-qemu @rm -rf build/ @rm -rf temp/ @cd kernel/ \u0026amp;\u0026amp; cargo clean @cd workspace/ \u0026amp;\u0026amp; make clean @cd fat32/ \u0026amp;\u0026amp; cargo clean @cd misc/ \u0026amp;\u0026amp; make clean fat32img: @cd kernel/ \u0026amp;\u0026amp; make fat32img run: @cd kernel/ \u0026amp;\u0026amp; make run debug-server: @cd kernel/ \u0026amp;\u0026amp; make debug-server debug: @cd kernel/ \u0026amp;\u0026amp; make debug 这里只需关注以下几点:\nrun: 构建内核并运行，内核是以 release 方式构建的 debug-server: 以 debug 方式构建内核并运行 gdb debug server debug: 链接上面运行的 debug server 开始调试 ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/about-makefile/","tags":["Makefile","Debug","区域赛"],"title":"[区域赛] 有关 BTD 的 Makefile"},{"content":"在 BTD 的开发中经常会遇到 debug 的需求，BTD 提供了一个方便的调试流程，只需启动两个 shell 即可 开始调试\n下面将演示一次具体的调试流程\n启动两个 shell，在项目根目录下分别运行 make debug-server 和 make debug， 之后只需要在运行 make debug 的 shell 中执行接下来的命令即可 # 从当前 pc 开始显示 11 条指令，11 条是不发生折叠的极限（也可能只是我这是这样） (gdb) x/11i $pc (gdb) b *0x80000000 # 在内核第一条指令处打个断点 (gdb) c # continue 执行到断点处 (gdb) si # si 单步执行 (step in，会嵌入函数具体流程中) 需要注意的是，在多核运行时，代码执行过程中会出现系统线程的切换，例如下面这样：\n(gdb) c Continuing. [Switching to Thread 1.2] 这时候我们要看当前线程的状态，根据当前状态进行调试，该打断点的地方不要忘了打断点，不然会跑飞\n像是这样，只能用 Ctrl + C 来掐死 (主要是我也没去找其他可能的方法 😛):\n看到地址了吗，直接归零，而且这块地址是不可访问的，只能卡在这了，除非扬了当前的 shell 重来（大概\nThread 2 received signal SIGINT, Interrupt. 0x0000000000000000 in ?? () (gdb) x/11i $pc =\u0026gt; 0x0: Cannot access memory at address 0x0 (gdb) ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/how-to-debug-the-kernel/","tags":["Debug"],"title":"如何调试 BTD-OS"},{"content":"内核在运行的时候总是会不知何时卡死，底层原因是持续触发时钟中断\n// 时间片到了 Trap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { set_next_trigger(); // 主要是这里 suspend_current_and_run_next(); } 初步解决 我们将设置下一个时钟中断放置在了 suspend_current_and_run_next 之前，导致可能因为后者执行 时间过长而使用户态一直处于时钟中断触发状态，至于为什么会在 RV64 上一直触发中断，可以参阅 RV 的特权级手册:\nPlatforms provide a real-time counter, exposed as a memory-mapped machine-mode read-write register, mtime. mtime must increment at constant frequency, and the platform must provide a mechanism for determining the period of an mtime tick. The mtime register will wrap around if the count overflows.\nThe mtime register has a 64-bit precision on all RV32 and RV64 systems. Platforms provide a 64- bit memory-mapped machine-mode timer compare register (mtimecmp). A machine timer interrupt becomes pending whenever mtime contains a value greater than or equal to mtimecmp, treating the values as unsigned integers. The interrupt remains posted until mtimecmp becomes greater than mtime (typically as a result of writing mtimecmp). The interrupt will only be taken if interrupts are enabled and the MTIE bit is set in the mie register.\n由于 suspend_current_and_run_next 执行的时间超过了一个时间片的长度，导致其返回用户态进程时， mtime 的值已经大于了 set_next_trigger 设置的时间点，由上文可得，如果 mtime 大于等于 mtimecmp(即 set_next_trigger 设置的值)，并且 mie 为使能状态，那么时钟中断会一直处于触发状态.\n而我们的内核 mie 一直处于使能状态，所以 S 态的时钟中断会持续在用户态发生(S 态中断不会打断同级与 更高特权级代码的执行)，导致用户态毫无进展，而我们内核的引导程序 initproc 会一直等待卡死用户进程 变为僵尸态，所以造成了内核执行流的卡死.\n解决办法:\n简单调整下位置\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); set_next_trigger(); } 但这样真的对吗? 不对，因为会导致用户态程序卡死整个内核的执行流\n一个致命的缺点是，用户态的程序需要第一次运行后才能正确的获取时钟中断，不然只能等轮回一边后才可能正确让出\n当前的逻辑是:\nRustSBI 完成初始化后，在 meow(没错，这是我们 Rust 代码的 ENTRYPOINT)，中初步设定一个时钟中断\n#[cfg(not(feature = \u0026quot;multi_harts\u0026quot;))] #[no_mangle] pub fn meow() -\u0026gt; ! { if hartid!() == 0 { init_bss(); unsafe { set_fs(FS::Dirty) } lang_items::setup(); logging::init(); mm::init(); trap::init(); trap::enable_stimer_interrupt(); trap::set_next_trigger(); fs::init(); task::add_initproc(); task::run_tasks(); } else { loop {} } unreachable!(\u0026quot;main.rs/meow: you should not be here!\u0026quot;); } 这是第一个问题，我们原本想的是，这个时钟中断会在第一用户态程序运行时发生，但是有可能它在 fs::init() 或者 task::add_initproc() 中已经发生了，这会导致一进入用户态程序就发生中断，这和我们 预期的不一样.\n而且，陷入中断后，除非使失能 mie，或者再次 set_next_trigger()(又或者 mtime 发生回环)， 否则将一直处于中断触发的状态\n而这之后切换的用户进程都会遇到中断而直接返回，直到运行到第一个用户进程(其实应该是引导程序 initproc)， 在下面 suspend_current_and_run_next 真正意义上的返回后，重新设置下一个中断时间点，这才能让 OS 内核 所有的用户进程进入正常的运行流.\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); set_next_trigger(); } 而这种时间上的开销显然是没必要的，所以我们根据所有用户进程都会通过 trap_return 返回用户态这一点， 将 set_next_trigger 设置在了 trap_return 中，同时判断当前进程是否是因为时间片耗尽而导致的 trap 返回:\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); } // ... #[no_mangle] pub fn trap_return() -\u0026gt; ! { // ... if is_time_intr_trap() { set_next_trigger(); } // ... } /// 是否是由于时间片耗尽导致的 trap fn is_time_intr_trap() -\u0026gt; bool { let scause = scause::read(); scause.cause() == Trap::Interrupt(scause::Interrupt::SupervisorTimer) } 最终结果 我们消除了一个严重的 bug: 内核在执行用户程序时随机卡死 删去了 meow 中不利于系统鲁棒性的代码 ","date":"2023-06-29","permalink":"https://bitethedisk.github.io/post/random-stuck/","tags":["问题与解决"],"title":"[已解决] 内核在执行用户程序时随机卡死"}]
>>>>>>> e2bdb38 (Tidy tags; Add new post/fs-optimization.md)
