[{"content":"","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/rust-and-c/","tags":["全国赛第一阶段","FFI"],"title":"Rust 和 C 的交互"},{"content":"在目前的实现中信号总共有63种，1-31为非实时信号，34-63是实时信号。32和33为未定义信号。 信号是每个进程独有的，除此之外每个进程还有信号掩码。 涉及信号处理的共有3个系统调用：SYS_SIGACTION，SYS_SIGPROCMASK，SYS_SIGRETURN。\n涉及的系统调用 sys_sigaction用于为一个信号注册信号处理函数，当进程接受到信号后会跳转到这个信号处理函数。一般信号处理函数后会调用sigreturn将程序上下文恢复到执行信号处理函数之前的状态。但程序也可能不调用sigreturn，而是使用longjump跳转到别的位置，内核不关心信号处理函数是否返回，不在内核中维护信号处理上下文信息，而是在执行信号处理函数之前将上下文压入用户栈，并在sigreturn时从用户栈中恢复信息。在执行信号处理函数的过程中，这些保存的上下文信息可能被用户程序修改，用户程序可以借此返回到不同的地方。\nsys_sigprocmask用于修改进程的信号掩码。信号掩码可以用来屏蔽信号，被屏蔽的信号会被阻塞，直到信号掩码不再阻塞该信号时该信号才会被处理。 9号信号和19号信号不能被阻塞。 sys_sigreturn会从用户栈中取出上下文信息将程序恢复到信号处理前的状态。 sys_sigreturn没有返回值，在执行完后不应将执行结果写入a0寄存器。\n由于执行信号处理函数时与执行其他用户函数没有区别，linux的信号设计自然支持信号处理的嵌套，只要正确实现了这几个系统调用，无需在内核内保存额外信息就可以支持信号的嵌套调用。 一般信号的产生是通过sys_kill等系统调用产生的，除此之外进程退出时也会产生信号。 信号处理的时机是不确定的。现在内核中会在每次返回用户态之前检查有无需要处理的信号。\n与其他系统调用的交互 sys_wait4 、sys_read、sys_futex等具有阻塞等待行为的系统调用可以被信号中断，如果在阻塞过程中有到来的信号应该停止等待，返回被中断的错误码。 fork 出来的子进程应该继承父进程的注册的信号处理程序，和信号掩码。 exec 后程序应该清空信号处理程序但是保留信号掩码。\n","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/signal/","tags":["全国赛第一阶段"],"title":"信号处理"},{"content":"由于大部分测试需要使用 busybox，为了避免多次解析 elf、从零创建地址空间等问题，我们采用了类似于加载initproc的方法。具体而言，我们将 busybox 预加载到内核中，并保存 load_elf 获取的信息。每次执行busybox时，我们直接使用保存的 load_elf 信息，并通过写时拷贝来创建所需的 busybox 进程的地址空间，更快速地创建 busybox 进程从而实现更高效的测试。\n// kernel/src/task/initproc/mod.rs pub static ref BUSYBOX: RwLock\u0026lt;Busybox\u0026gt; = RwLock::new({ extern \u0026quot;C\u0026quot; { fn busybox_entry(); fn busybox_tail(); } let entry = busybox_entry as usize; let tail = busybox_tail as usize; let siz = tail - entry; let busybox = unsafe { core::slice::from_raw_parts(entry as *const u8, siz) }; let path = AbsolutePath::from_str(\u0026quot;/busybox0\u0026quot;); let inode = fs::open(path, OpenFlags::O_CREATE, CreateMode::empty()).expect(\u0026quot;busybox0 create failed\u0026quot;); inode.write_all(\u0026amp;busybox.to_owned()); let bb = Arc::new(TaskControlBlock::new(inode.clone())); inode.delete(); Busybox { inner: bb, } }); pub static mut ONCE_BB_ENTRY: usize = 0; pub static mut ONCE_BB_AUX: Vec\u0026lt;AuxEntry\u0026gt; = Vec::new(); pub struct Busybox { inner: Arc\u0026lt;TaskControlBlock\u0026gt;, } impl Busybox { pub fn elf_entry_point(\u0026amp;self) -\u0026gt; usize { unsafe { ONCE_BB_ENTRY } } pub fn aux(\u0026amp;self) -\u0026gt; Vec\u0026lt;AuxEntry\u0026gt; { unsafe { ONCE_BB_AUX.clone() } } pub fn memory_set(\u0026amp;self) -\u0026gt; MemorySet { let mut write = self.inner.memory_set.write(); MemorySet::from_copy_on_write(\u0026amp;mut write) } } ","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/busybox-pre-init/","tags":["busybox","全国赛I"],"title":"[全国赛I] busybox 预加载"},{"content":"在我们的内核中，我们使用 TASKMANAGER 管理分别处于就绪态，阻塞态的进程，包括因为调用 nanosleep 而休眠的进程。\n// kernel/src/task/manager.rs // 负责管理待调度的进程对象 pub struct TaskManager { ready_queue: VecDeque\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, waiting_queue: VecDeque\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, hq: BinaryHeap\u0026lt;HangingTask\u0026gt;, } // 用于管理 sleep 进程 pub struct HangingTask { wake_up_time: usize, // ns inner: Arc\u0026lt;TaskControlBlock\u0026gt;, } // 用于处理子线程的资源回收 pub struct ChildrenThreadMonitor { cancelled_child_threads: Vec\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, } // 维护内核中 pid 到 TCB 的映射 pub static ref PID2TCB: Mutex\u0026lt;BTreeMap\u0026lt;usize, Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;\u0026gt; = Mutex::new(BTreeMap::new()); // 子线程回收管理器 pub static CHILDREN_THREAD_MONITOR: Mutex\u0026lt;ChildrenThreadMonitor\u0026gt; = Mutex::new(ChildrenThreadMonitor::new()); // kernel/src/task/processor/processor.rs /// [`Processor`] 是描述 CPU执行状态 的数据结构。 /// 在单核环境下，我们仅创建单个 Processor 的全局实例 PROCESSOR pub static mut PROCESSOR: SyncRefCell\u0026lt;Processor\u0026gt; = SyncRefCell::new(Processor::new()); /// 每个核上的处理器，负责运行一个进程 pub struct Processor { /// 当前处理器上正在执行的任务 current: Option\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt;, /// 当前处理器上的 idle 控制流的任务上下文 idle_task_cx: TaskContext, } run_tasks 分别尝试从 hang_task, ready_task 队列中获取进程调度。\n// kernel/src/task/processor/schedule.rs /// 进入 idle 控制流，它运行在这个 CPU 核的启动栈上， /// 功能是循环调用 fetch_task 直到顺利从任务管理器中取出一个任务，随后便准备通过任务切换的方式来执行 pub fn run_tasks() { let bb = BUSYBOX.read(); // lazy static busybox drop(bb); loop { let processor = acquire_processor(); recycle_child_threads_res(); if let Some(hanging_task) = check_hanging() { run_task(hanging_task, processor); } else if let Some(interupt_task) = check_futex_interupt_or_expire() { unblock_task(interupt_task); } else if let Some(task) = fetch_task() { run_task(task, processor); } } } 值得一提的是，我们在进程调度时还需要检测 block_task 队列中，因为在系统调用过程中被信号打断的 task 是否有处理完信号，或者 futex_wait 时给出的 timeout 是否已超时以唤醒该进程并加入到 ready_task 中。\n// kernel/src/task/manager.rs pub fn check_futex_interupt_or_expire(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Arc\u0026lt;TaskControlBlock\u0026gt;\u0026gt; { for tcb in self.waiting_queue.iter() { let lock = tcb.inner_ref(); // 被信号打断的 task 是否有处理完信号 if !lock.pending_signals.difference(lock.sigmask).is_empty() { return Some(tcb.clone()); } } let mut global_futex_que = FUTEX_QUEUE.write(); for (_, futex_queue) in global_futex_que.iter_mut() { // timeout 是否已超时 if let Some(task) = futex_queue.pop_expire_waiter() { return Some(task.clone()); } } None } ","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/scheduling/","tags":["process","thread","全国赛I"],"title":"[全国赛I] 进程/线程调度"},{"content":"TCB 与 fork 的更改 相较于初赛，完善了 clone 系统调用。初赛时由于要求测试样例要求比较低，在实现 clone 系统调用时并未完全利用用户传递的参数。我们根据 Linux manual page 中的要求，完善了内核的 fork 以及 TaskControlBlock 结构。\n// kernel/task/task.rs pub struct TaskControlBlock { ... pub sigactions: Arc\u0026lt;RwLock\u0026lt;[SigAction; MAX_SIGNUM as usize]\u0026gt;\u0026gt;, pub memory_set: Arc\u0026lt;RwLock\u0026lt;MemorySet\u0026gt;\u0026gt;, pub fd_table: Arc\u0026lt;RwLock\u0026lt;FDTable\u0026gt;\u0026gt;, pub robust_list: Arc\u0026lt;RwLock\u0026lt;RobustList\u0026gt;\u0026gt;, pub rlimit_nofile: Arc\u0026lt;RwLock\u0026lt;RLimit\u0026gt;\u0026gt;, inner: RwLock\u0026lt;TaskControlBlockInner\u0026gt;, } pub struct TaskControlBlockInner { ... pub pending_signals: SigSet, pub sigmask: SigMask, pub interval_timer: Option\u0026lt;IntervalTimer\u0026gt;, pub utime: TimeVal, pub stime: TimeVal, pub last_enter_umode_time: TimeVal, pub last_enter_smode_time: TimeVal, pub clear_child_tid: usize, /* CLONE_CHILD_CLEARTID */ } 相较于初赛，我们为 TCB 加入了有关信号、时间、资源等结构。并根据 sys_clone 传递的参数，正确地实现 fork，比如以下代码段：\n// kernel/src/task/task.rs(fn fork) // 拷贝用户地址空间 let memory_set = if flags.contains(CloneFlags::VM) { self.memory_set.clone() } else { Arc::new(RwLock::new(MemorySet::from_copy_on_write( \u0026amp;mut self.memory_set.write(), ))) }; if flags.contains(CloneFlags::THREAD) { memory_set.write().map_thread_trap_context(private_tid); } // kernel/syscall/impls/process.rs (sys_do_fork) if flags.contains(CloneFlags::PARENT_SETTID) { *translated_mut(current_user_token(), ptid as *mut u32) = new_pid as u32; } if flags.contains(CloneFlags::CHILD_SETTID) { *translated_mut(child_token, ctid as *mut u32) = new_pid as u32; } if flags.contains(CloneFlags::CHILD_CLEARTID) { new_task.inner_mut().clear_child_tid = ctid; } if flags.contains(CloneFlags::SETTLS) { let trap_cx = new_task.inner_mut().trap_context(); trap_cx.set_tp(tls); } 线程的引入 在 fork 过程中，当 CloneFlags 中存在 CLONE_THREAD 位时，正在创建的进程当前进程的为子线程\nPid 分配器的更改\n由于我们的内核在实现线程时，为了更方便地为子线程分配 TrapContext Frame 资源，我们规定子线程的 tid (pid) 不应该与小于进程（主线程）的 tid (pid) ，故移除了进程 pid 的回收操作。\n子进程与子线程的区分\n目前我们将子线程与子进程均保存在 TCB 的 children 字段，在遇到进程退出等问题时会判断 child 是子进程还是子线程。\n目前通过 tgid 与 pid 来区分 TCB 是父进程的子进程还是子线程\n// kernel/src/task/task.rs(fn fork) let pid_handle = pid_alloc(); let tgid = if flags.contains(CloneFlags::THREAD) { self.pid.0 } else { pid_handle.0 }; 如果 tgid 与 pid 值相同，则该 TCB 为进程，否则为线程\n为线程分配资源\n线程除了共享主线程（进程）的 memory_set, fd_table, sigaction 等资源，还需要一些独立的资源如ID, 内核地址空间的KernelStack，以及主线程中独立分配的 TrapContext Frame:\n// kernel/syscall/impls/process.rs (sys_do_fork) if flags.contains(CloneFlags::THREAD) { memory_set.write().map_thread_trap_context(private_tid); } // kernel/src/task/ttask.rs pub fn trap_context_position(tid: usize) -\u0026gt; VirtAddr { VirtAddr::from(TRAP_CONTEXT - tid * PAGE_SIZE) } 其中，private_tid 为tgid(主线程/父进程)与pid(子线程tid)的差值\n进程/线程的退出 当前进程结束的方式包括：\n进程运行完代码段，非法访问到 .rodata 引发 trap，在 trap 处理中回收进程对象 进程调用 exit 系统调用 进程收到 kill 相关的信号，在信号处理时退出 关于进程/线程的退出时需要做的工作包括：\n完成进程的初步回收： 将自身从 PID2TCB 映射管理器中移除 标记自身状态为 Zombie，记录退出码 将子进程移交给 initproc 如果自身为某进程的子线程，还需要找到主进程并将自身从主进程中移除，并压入子线程回收管理器 CHILDREN_THREAD_MONITOR 中，在下一次进程调度时回收可回收的资源 进程的父进程等待子进程退出，调用 wait 系统调用，完成子进程资源的回收：找到子进程中处于 Zombie 态的进程并且强引用计数为 1 的进程，移除该进程以彻底回收该进程的所有资源 子线程资源回收\n子线程退出时，子线程会加入到回收管理器 CHILDREN_THREAD_MONITOR 中，并在下一次进程调度时回收可回收的资源。\n// kernel/src/task/mod.rs (fn exit_current_and_run_next) if is_child_thread { let parent = inner.parent.as_ref().unwrap().upgrade().unwrap(); let mut parent_inner = parent.inner_mut(); let children_iter = parent_inner.children.iter(); let (idx, _) = children_iter .enumerate() .find(|(_, t)| t.pid() == pid) .unwrap(); parent_inner.children.remove(idx); drop(parent_inner); drop(parent); drop(inner); assert!(Arc::strong_count(\u0026amp;task) == 1); take_cancelled_chiled_thread(task); schedule(\u0026amp;mut TaskContext::empty() as *mut _); unreachable!() } 这个过程本身其实可以不用做，而是等主线程进行 wait 系统调用时彻底回收。但由于测试过程中，进程会创建成千上万个子线程，如果这些线程资源没有及时回收，如 TrapContext, KenerlStack 等资源，会浪费许多内存资源。\n其实 take_cancelled_chiled_thread(task)这段代码本身，以及 CHILDREN_THREAD_MONITOR变量，也就是说这段代码本身其实可以直接改为 drop(task)，因为此时 task 强引用计数一定为 1，task 中可以释放的资源都可以在, schedule 之前释放掉但是 task 在执行 exit_current_and_run_next时本身出于内核态，此时回收 task 的 KerenlStack 既不符合逻辑，又有可能产生一些隐患，故选择使用 CHILDREN_THREAD_MONITOR 在调度时释放退出的线程。\n","date":"2023-08-01","permalink":"https://bitethedisk.github.io/post/process-thread/","tags":["process","thread","全国赛I"],"title":"[全国赛I] 进程管理-进程与线程"},{"content":"一开始\n","date":"2023-07-12","permalink":"https://bitethedisk.github.io/post/opensbi-getchar/","tags":["全国赛I"],"title":"适配 OpenSBI"},{"content":"Rust 和 C 交互的时候需要注意 Rust 的堆，当从 Rust 程序 fork 出一个进程 来执行 C 程序的时候，C 是不能识别 Rust 的堆的(包括其堆管理器这些)，所以当 使用 C 程序去访问 fork 后的 Rust 堆上的对象，或者传递相关堆上对象的时候可能 会产生致命问题(如非法内存访问\u0026mdash;访问越界)\n","date":"2023-07-07","permalink":"https://bitethedisk.github.io/post/rust-heap-and-c/","tags":null,"title":"Rust Heap and C"},{"content":"随着开发的进行，我们需要的适配和封装的数据结构越来越多，其中大部分与我们的内核本体关系并没有那么紧密， 所以我们将这部分结构，如用于的引导程序、FAT32、Linux 相关数据结构放在了项目根目录中的 crates 里\nRust 本身是支持多个 crates 构成的一个 workspace，这些 crates 直接可以相互引用，但是由于我们使用 了 .cargo/config.toml 来配置 rustc，所以 workspace 并不能为我们所有 (因为目前 workspace 不支持在 workspace 中读取 .cargo/config.toml)\n使用 Git Submodule 管理测例 与区域赛不同，全国赛的测例数目较多，如果一旦发生更新构建起来也相对麻烦\n基于 Git Submodule 我们可以方便隔离当前 Git 仓库，做到依赖的隔离与同步\n就当前的实际环境来说:\ngit submodule add https://github.com/oscomp/testsuits-for-oskernel.git testsuits 上面的作用是将 testsuits-for-oskernel.git clone 到本地的 testsuits 文件夹中，后者会自动创建\n当重新拉取项目仓库时:\ngit submodule init git submodule update 就可以重新拉取 testsuits 中，仓库的内容了\n项目目录树 . ├── Makefile ├── README.md ├── crates │ ├── fat32/ ---- FAT32 读写库 │ ├── libd/ ---- libc 的~~后继者(划掉)~~ initproc，内核自动加载的第一个用户程序 │ ├── nix/ ---- Linux 相关数据结构 │ └── sync_cell/ ---- 实现了 Sync 的，具有内部可变性的 RefCell ├── docs/ ├── kernel/ │ ├── Makefile │ ├── build.rs ---- 用于监控相关文件，如 `crates/libd/bin/initproc.rs`，发生变化时重新编译 │ ├── cargo │ │ └── config.toml │ ├── linkerld │ │ └── linker.ld │ ├── src │ │ ├── boards │ │ │ └── qemu.rs ---- 平台相关参数 │ │ ├── console.rs │ │ ├── consts.rs │ │ ├── drivers │ │ ├── entry.S │ │ ├── error.rs │ │ ├── fs/ │ │ ├── logging.rs │ │ ├── macros │ │ │ ├── hsm.rs │ │ │ ├── mod.rs │ │ │ ├── on_boot.rs │ │ │ └── profile.rs ---- 用于打印某段代码运行时间的宏 │ │ ├── main.rs │ │ ├── mm │ │ ├── sbi.rs │ │ ├── syscall/ │ │ ├── task/ │ │ ├── timer.rs │ │ └── trap/ │ ├── target/ ---- 构建产物 │ └── vendor/ ---- 所有第三方依赖的本地归档 ├── testsuits/ ---- 通过 Git Submodule 内联的官方测例 └── workspace ---- 用于中间过程构建内核运行所需测例 1187 directories, 9162 files ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/project-structure/","tags":["全国赛I","RustWorkspace"],"title":"[全国赛I]项目结构(rust-workspace不能使用)"},{"content":"项目提供了一系列的 Makefile 来简化开发流程.\n一般只会用到项目根目录中的 Makefile:\nBOOTLOADER_ELF = ./kernel/bootloader/rustsbi-qemu KERNEL_ELF = ./kernel/target/riscv64gc-unknown-none-elf/release/kernel sbi-qemu: @cp $(BOOTLOADER_ELF) sbi-qemu kernel-qemu: @mv kernel/cargo kernel/.cargo @cd kernel/ \u0026amp;\u0026amp; make kernel @cp $(KERNEL_ELF) kernel-qemu all: sbi-qemu kernel-qemu clean: @rm -f kernel-qemu @rm -f sbi-qemu @rm -rf build/ @rm -rf temp/ @cd kernel/ \u0026amp;\u0026amp; cargo clean @cd workspace/ \u0026amp;\u0026amp; make clean @cd fat32/ \u0026amp;\u0026amp; cargo clean @cd misc/ \u0026amp;\u0026amp; make clean fat32img: @cd kernel/ \u0026amp;\u0026amp; make fat32img run: @cd kernel/ \u0026amp;\u0026amp; make run debug-server: @cd kernel/ \u0026amp;\u0026amp; make debug-server debug: @cd kernel/ \u0026amp;\u0026amp; make debug 这里只需关注以下几点:\nrun: 构建内核并运行，内核是以 release 方式构建的 debug-server: 以 debug 方式构建内核并运行 gdb debug server debug: 链接上面运行的 debug server 开始调试 ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/about-makefile/","tags":["Makefile","Debug","区域赛"],"title":"[区域赛] 有关 BTD 的 Makefile"},{"content":"在 BTD 的开发中经常会遇到 debug 的需求，BTD 提供了一个方便的调试流程，只需启动两个 shell 即可 开始调试\n下面将演示一次具体的调试流程\n启动两个 shell，在项目根目录下分别运行 make debug-server 和 make debug， 之后只需要在运行 make debug 的 shell 中执行接下来的命令即可 # 从当前 pc 开始显示 11 条指令，11 条是不发生折叠的极限（也可能只是我这是这样） (gdb) x/11i $pc (gdb) b *0x80000000 # 在内核第一条指令处打个断点 (gdb) c # continue 执行到断点处 (gdb) si # si 单步执行 (step in，会嵌入函数具体流程中) 需要注意的是，在多核运行时，代码执行过程中会出现系统线程的切换，例如下面这样：\n(gdb) c Continuing. [Switching to Thread 1.2] 这时候我们要看当前线程的状态，根据当前状态进行调试，该打断点的地方不要忘了打断点，不然会跑飞\n像是这样，只能用 Ctrl + C 来掐死 (主要是我也没去找其他可能的方法 😛):\n看到地址了吗，直接归零，而且这块地址是不可访问的，只能卡在这了，除非扬了当前的 shell 重来（大概\nThread 2 received signal SIGINT, Interrupt. 0x0000000000000000 in ?? () (gdb) x/11i $pc =\u0026gt; 0x0: Cannot access memory at address 0x0 (gdb) ","date":"2023-06-30","permalink":"https://bitethedisk.github.io/post/how-to-debug-the-kernel/","tags":["OSKernel","Debug"],"title":"开发中如调试内核"},{"content":"内核在运行的时候总是会不知何时卡死，底层原因是持续触发时钟中断\n// 时间片到了 Trap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { set_next_trigger(); // 主要是这里 suspend_current_and_run_next(); } 初步解决 我们将设置下一个时钟中断放置在了 suspend_current_and_run_next 之前，导致可能因为后者执行 时间过长而使用户态一直处于时钟中断触发状态，至于为什么会在 RV64 上一直触发中断，可以参阅 RV 的特权级手册:\nPlatforms provide a real-time counter, exposed as a memory-mapped machine-mode read-write register, mtime. mtime must increment at constant frequency, and the platform must provide a mechanism for determining the period of an mtime tick. The mtime register will wrap around if the count overflows.\nThe mtime register has a 64-bit precision on all RV32 and RV64 systems. Platforms provide a 64- bit memory-mapped machine-mode timer compare register (mtimecmp). A machine timer interrupt becomes pending whenever mtime contains a value greater than or equal to mtimecmp, treating the values as unsigned integers. The interrupt remains posted until mtimecmp becomes greater than mtime (typically as a result of writing mtimecmp). The interrupt will only be taken if interrupts are enabled and the MTIE bit is set in the mie register.\n由于 suspend_current_and_run_next 执行的时间超过了一个时间片的长度，导致其返回用户态进程时， mtime 的值已经大于了 set_next_trigger 设置的时间点，由上文可得，如果 mtime 大于等于 mtimecmp(即 set_next_trigger 设置的值)，并且 mie 为使能状态，那么时钟中断会一直处于触发状态.\n而我们的内核 mie 一直处于使能状态，所以 S 态的时钟中断会持续在用户态发生(S 态中断不会打断同级与 更高特权级代码的执行)，导致用户态毫无进展，而我们内核的引导程序 initproc 会一直等待卡死用户进程 变为僵尸态，所以造成了内核执行流的卡死.\n解决办法:\n简单调整下位置\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); set_next_trigger(); } 但这样真的对吗? 不对，因为会导致用户态程序卡死整个内核的执行流\n一个致命的缺点是，用户态的程序需要第一次运行后才能正确的获取时钟中断，不然只能等轮回一边后才可能正确让出\n当前的逻辑是:\nRustSBI 完成初始化后，在 meow(没错，这是我们 Rust 代码的 ENTRYPOINT)，中初步设定一个时钟中断\n#[cfg(not(feature = \u0026quot;multi_harts\u0026quot;))] #[no_mangle] pub fn meow() -\u0026gt; ! { if hartid!() == 0 { init_bss(); unsafe { set_fs(FS::Dirty) } lang_items::setup(); logging::init(); mm::init(); trap::init(); trap::enable_stimer_interrupt(); trap::set_next_trigger(); fs::init(); task::add_initproc(); task::run_tasks(); } else { loop {} } unreachable!(\u0026quot;main.rs/meow: you should not be here!\u0026quot;); } 这是第一个问题，我们原本想的是，这个时钟中断会在第一用户态程序运行时发生，但是有可能它在 fs::init() 或者 task::add_initproc() 中已经发生了，这会导致一进入用户态程序就发生中断，这和我们 预期的不一样.\n而且，陷入中断后，除非使失能 mie，或者再次 set_next_trigger()(又或者 mtime 发生回环)， 否则将一直处于中断触发的状态\n而这之后切换的用户进程都会遇到中断而直接返回，直到运行到第一个用户进程(其实应该是引导程序 initproc)， 在下面 suspend_current_and_run_next 真正意义上的返回后，重新设置下一个中断时间点，这才能让 OS 内核 所有的用户进程进入正常的运行流.\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); set_next_trigger(); } 而这种时间上的开销显然是没必要的，所以我们根据所有用户进程都会通过 trap_return 返回用户态这一点， 将 set_next_trigger 设置在了 trap_return 中，同时判断当前进程是否是因为时间片耗尽而导致的 trap 返回:\nTrap::Interrupt(Interrupt::SupervisorTimer) =\u0026gt; { suspend_current_and_run_next(); } // ... #[no_mangle] pub fn trap_return() -\u0026gt; ! { // ... if is_time_intr_trap() { set_next_trigger(); } // ... } /// 是否是由于时间片耗尽导致的 trap fn is_time_intr_trap() -\u0026gt; bool { let scause = scause::read(); scause.cause() == Trap::Interrupt(scause::Interrupt::SupervisorTimer) } 最终结果 我们消除了一个严重的 bug: 内核在执行用户程序时随机卡死 删去了 meow 中不利于系统鲁棒性的代码 ","date":"2023-06-29","permalink":"https://bitethedisk.github.io/post/random-stuck/","tags":["Problems-\u0026-Solutions"],"title":"[已解决]内核在执行用户程序时随机卡死"}]